---
title: "Beyond 50K"
subtitle: "Unleashing Predictive Insights into Income Classification with Machine Learning"
author: "Dominik Zabinski 306068"
output: 
    beamer_presentation:
        theme: "Goettingen"
        colortheme: "dolphin"
        fonttheme: "structurebold"
header-includes:
- \usepackage{booktabs}
- \usepackage[table]{xcolor}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(kableExtra)
```

# Agenda

- Problem definition
- Data analysis
- Data preparation
- Methods
- Results
- Summary

# Problem definition

- Which features are most important while predicting salary?
- What is the best model for this task?
- How good is the best model?

# Data analysis

![Salary distribution](306068_classification_report_files/figure-html/y_distribution-1.png)

#

```{r}
readRDS("class_table_descr_cont.rds") %>% 
    mutate(min = round(min, 2), mean = round(mean, 2), sd = round(sd, 2), max = round(max, 2)) %>% 
    kable(booktabs = T, format = "latex", 
          col.names = c("Variable", "Min", "Avg", "Sd", "Max", "# miss.", "# dist.")) %>%
    kable_styling(latex_options = "striped", font_size = 7) %>% 
    kableExtra::column_spec(column = 1, width = "4em") %>% 
    row_spec(row = 4, color = "red")
```

#

![Correlation matrix between continous variables](306068_classification_report_files/figure-html/corrplot-1.png)

#

```{r}
readRDS("class_table_descr_disc.rds") %>% 
    kable(booktabs = T, format = "latex", 
          col.names = c("Variable", "# dist.", "# miss.", "Top levels", "Dist. range", "# range")) %>%
    kable_styling(latex_options = "striped", font_size = 7) %>% 
    kableExtra::column_spec(column = 4, width = "11em") %>% 
    kableExtra::column_spec(column = 5, width = "3em") %>% 
    kableExtra::column_spec(column = 6, width = "2em") %>% 
    row_spec(row = 1, color = "red") %>% 
    row_spec(row = c(2:4, 8), color = "blue")
```


# Data preparation

- joining small groups in discrete variable (data-driven and/or gut feeling)
- one-hot encoding discrete variables
- split training/testing in 70/30 proportion
- normalize continuous variables
- 5-fold cross-validation as sampling method

# 

![Dendrogram for 'native.country' variable](306068_classification_report_files/figure-html/native_dendr-1.png)


# Methods

Single models:

- Decision tree
- Random forest
- Generalized Boosted Regression Modeling (GBM)
- eXtreme Gradient Boosting (XGBoost)
- Logistic regression (as benchmark)

Ensemble model: weighted combination of single models

Stacked models:

- Logistic Regression as top layer model
- XGBoost as top layer model

Assessment metric: AUC

# Results

![Comparison of ROC curves - data type](306068_classification_report_files/figure-html/comparison_by_dataset-1.png)

# 

![Comparison of ROC curves - model type](306068_classification_report_files/figure-html/comparison_by_model-1.png)

#

```{r}
summ_up_table <- readRDS("class_summ_up_table.rds")

summ_up_table %>% 
    mutate(auc_train = scales::percent(auc_train),
           auc_test = scales::percent(auc_test),
           gini_train = scales::percent(gini_train),
           gini_test = scales::percent(gini_test)) %>% 
    kable(booktabs = T, format = "latex", 
          col.names = c("Model", "AUC (Train)", "AUC (Test)", "Gini (Train)", "Gini (Test)")) %>%
    kable_styling(font_size = 7) %>%
    row_spec(which(summ_up_table$auc_train == max(summ_up_table$auc_train)), bold = T, color = "white", background = "green") %>% 
    row_spec(which(summ_up_table$auc_test == max(summ_up_table$auc_test)), underline = T)
```

#

![Variable importance - ensemble model](306068_classification_report_files/figure-html/var_imp_ensemble-1.png)

# Summary

- Which features are most important while predicting salary?
    - being married and living with a spouse
    - capital gain
    - age
    - feat02, feat04 and feat06
    - higher education (BA+)
- What is the best model for this task?
    - Stacked with XGBoost as top layer model, 
    - **on production** I'd consider single XGBoost
- How good is the best model?
    - AUC: 94%
- What could be done differently?
    - hyperparameter tuning
    - binning continuous variables
