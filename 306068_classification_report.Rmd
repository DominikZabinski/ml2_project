---
title: "Salary Prediction Classification"
subtitle: "Classification on Salary whether less than 50K or greater than 50K"
output: 
    html_document:
        toc: true
        toc_float: true
        smooth_scroll: true
---

```{r setup, include=FALSE}
# libraries
library(tidyverse) 
library(corrplot) 
library(kableExtra) 
library(caret)  
library(caretEnsemble) 
library(rpart)
library(randomForest)
library(gbm)
library(xgboost)
library(pROC)

# settings
knitr::opts_chunk$set(cache = FALSE)
options(scipen = 999)

# functions
source("__functions.R")
```

```{r, reading_data}
data_raw <- read_csv(file = "c3.csv", show_col_types = FALSE)
```

# Description of data and problem
## Data
1994 Census database. Quick look at the dataset structure.

```{r, glimpse_at_data}
glimpse(data_raw)
```

42,651 observations, 26 variables.

## Problem
Predict whether a person makes over 50,000 a year.

# Descriptive analyses

Establish dependent variable (**y_variable**) and predictors (**x_variables**).

```{r, data_cleaning}
# Change the column names to avoid possible problems in the future
names(data_raw) <- make.names(names(data_raw))
# 'salary' as dependent variable
y_variable <- "salary"
# rest (excluding 'id') are predictors
x_variables <- setdiff(names(data_raw), c("id", "salary"))
```

Quick look at the dependent variable:

```{r, y_distribution}
data_raw %>% 
    group_by_at(y_variable) %>% 
    summarise(count_n = n()) %>% 
    mutate(m = paste0(.data[[y_variable]], " (", round(100 * count_n / sum(count_n), 1), "%)")) %>% 
    ggplot(mapping = aes(x = 1)) + 
    geom_col(mapping = aes(fill = m, y = count_n), 
             position = "stack", color = "white", linewidth = 2) +
    coord_polar(theta = "y") +
    labs(fill = y_variable, 
         title = "Distribution of dependent variable") +
    theme_minimal() +
    scale_fill_viridis_d() +
    theme(panel.grid = element_blank(), 
          axis.text = element_blank(), 
          axis.title = element_blank())
```

Ii is not balanced but no need for over/under sampling.

Levels contains special characters - remove them (it will cause problems when trying to get predicted probabilities).

```{r, y_variable_transform}
y_variable_values_adj <- rep("atmost50K", times = nrow(data_raw))
y_variable_values_adj[data_raw[[y_variable]] == ">50K"] <- "morethan50K"
data_raw[[y_variable]] <- y_variable_values_adj
```

Split variable into continuous and discrete variables:

```{r, split_vars}
x_variables_cont <- x_variables[sapply(X = x_variables, FUN = function(x) is(data_raw[[x]], "numeric"))]
x_variables_disc <- x_variables[!sapply(X = x_variables, FUN = function(x) is(data_raw[[x]], "numeric"))]
```

Take a look at continuous variables:

```{r, descr_cont}
table_descr_cont <- do.call(what = rbind, args = lapply(X = x_variables_cont, FUN = function(i) describe_cont_var(data_raw, i)))

saveRDS(object = table_descr_cont, file = "class_table_descr_cont.rds")

table_descr_cont %>% 
    mutate(min = round(min, 2), mean = round(mean, 2), sd = round(sd, 2), max = round(max, 2)) %>% 
    kable(booktabs = T) %>%
    kable_styling()
```

- no need for imputation (no missing records)
- **education.num** has 16 distinct values - might be good idea to change it to discrete

Discrete variables

```{r, descr_disc}
table_descr_disc <- do.call(rbind, lapply(X = x_variables_disc, FUN = function(i) describe_disc_var(data_raw, i)))

saveRDS(object = table_descr_disc, file = "class_table_descr_disc.rds")

table_descr_disc %>% 
    kable(booktabs = T) %>%
    kable_styling()
```

- **education** has 16 levels - same as **education.num** - possible duplicate of information
- **marital.status**, **native.country**, **occupation** and **workclass* has levels with less than 30 observations - those might cause problems with resampling

Examine further **education** and **education.num** variables

```{r, check_education}
data_raw %>% 
    group_by(education, education.num) %>% 
    summarise(count = n()) %>% 
    arrange(education.num) %>% 
    kable(booktabs = T) %>% 
    kable_styling()
```

Loose the **education.num** variable

```{r, remove_education}
x_variables_cont <- setdiff(x_variables_cont, "education.num")
```

Examine variables with low groups - marital.status

```{r, marital_exam}
marital_sum <- data_raw %>% 
    group_by_at("marital.status") %>% 
    summarise(count_n = n(), dist = sum(salary == "morethan50K") / count_n)
marital_sum %>% 
  kable() %>%
  kable_styling() %>%
  row_spec(which(marital_sum$count_n < 30), bold = T, color = "white", background = "red")
```

Since Married-AF-spouse has similar percentage of observations earning > 50K we join those two together

```{r, marital_transform}
data_raw <- data_raw %>% 
    mutate(marital.status2 = case_when(marital.status %in% c("Married-AF-spouse", "Married-civ-spouse") ~ "Married-civ-AF-spouse",
                                      .default = marital.status))
x_variables_disc <- c(setdiff(x_variables_disc, "marital.status"), "marital.status2")
```

What about **native.country**?

```{r, native_exam}
native_sum <- data_raw %>% 
    group_by_at("native.country") %>% 
    summarise(count_n = n(), dist = sum(salary == "morethan50K") / count_n)
native_sum %>% 
  kable() %>%
  kable_styling() %>%
  row_spec(which(native_sum$count_n < 30), bold = T, color = "white", background = "red")
```

There are some low levels:

- Cambodia
- Holand-Netherlands
- Honduras
- Hong
- Hungary
- Laos
- Outlying-US(Guam-USVI-etc)
- Scotland
- Thailand
- Trinidad&Tobago
- Yugoslavia

```{r, native_dendr}
native_cluster <- as.data.frame(native_sum)
native_suff <- substr(native_cluster$native.country, 1, 4)
native_low_level <- native_cluster %>% 
    filter(count_n < 30) %>% .$native.country
native_low_level_idx <- which(native_cluster$native.country %in% native_low_level)
native_suff[native_low_level_idx] <- paste0("**", native_suff[native_low_level_idx], "**")
row.names(native_cluster) <- paste(native_suff, 
                                   native_cluster$count_n, 
                                   round(100 * native_cluster$dist, 1), sep = "-")
par(cex = .6)
native_cluster %>% 
    select(dist) %>% 
    dist() %>% 
    hclust(., method = "average") %>% 
    plot()
par(cex = 1)
```

Not very helpful, resorting to experts knowledge

```{r, native_transform}
data_raw <- data_raw %>% 
    mutate(native.country2 = case_when(native.country %in% c("Thailand", "Laos") ~ "Thailand_Laos",
                                       native.country %in% c("Cambodia", "Philippines") ~ "Cambodia_Philippines",
                                       native.country %in% c("Honduras", "Jamaica") ~ "Honduras_Jamaica",
                                       native.country %in% c("Trinadad&Tobago", "Haiti") ~ "TT_Haiti",
                                       native.country %in% c("Hungary", "Poland", "Yugoslavia") ~ "Hungary_Poland_Yugoslavia",
                                       native.country %in% c("Scotland", "England") ~ "Brits",
                                       native.country %in% c("?", "Holand-Netherlands", "Outlying-US(Guam-USVI-etc)") ~ "Other",
                                       .default = native.country))
x_variables_disc <- c(setdiff(x_variables_disc, "native.country"), "native.country2")
```

Occupation

```{r, occupation_exam}
occupation_sum <- data_raw %>% 
    group_by_at("occupation") %>% 
    summarise(count_n = n(), dist = sum(salary == "morethan50K") / count_n)
occupation_sum %>% 
  kable() %>%
  kable_styling() %>%
  row_spec(which(occupation_sum$count_n < 30), bold = T, color = "white", background = "red")
```

Join **Armed-Forces** with **Protective-serv** and **?** with **Other-service**

```{r, occupation_transform}
data_raw <- data_raw %>% 
    mutate(occupation2 = case_when(occupation %in% c("Armed-Forces", "Protective-serv") ~ "Protection",
                                   occupation %in% c("?", "Other-service") ~ "Other",
                                   .default = occupation))
x_variables_disc <- c(setdiff(x_variables_disc, "occupation"), "occupation2")
```

**workclass**

```{r, workclass_exam}
workclass_sum <- data_raw %>% 
    group_by_at("workclass") %>% 
    summarise(count_n = n(), dist = sum(salary == "morethan50K") / count_n)
workclass_sum %>% 
  kable() %>%
  kable_styling() %>%
  row_spec(which(workclass_sum$count_n < 30), bold = T, color = "white", background = "red")
```

Join **Never-worked** and **Without-pay**

```{r, workclass_transform}
data_raw <- data_raw %>% 
    mutate(workclass2 = case_when(workclass %in% c("Never-worked", "Without-pay") ~ "NoIncome",
                                   .default = workclass))
x_variables_disc <- c(setdiff(x_variables_disc, "workclass"), "workclass2")
```

Just to make sure all the transformation went as expected.

```{r, descr_disc_after}
do.call(rbind, lapply(X = x_variables_disc, FUN = function(i) describe_disc_var(data_raw, i)))
```

Still 2 variables has < 30 in some levels but we are OK with it.

Is there a correlation between continuous variables?

```{r, corrplot}
corrplot::corrplot(corr = cor(data_raw[, x_variables_cont]), method = "color")
```

No clear correlation between continuous variables.

# Data preparation

## Variable transformations

Perform one-hot encoding for discrete variables:

```{r, one_hot}
one_hot_data <- data.frame(predict(object = dummyVars(formula = " ~ .", data = data_raw[x_variables_disc]), 
                                   newdata = data_raw[x_variables_disc]))
```

Bind all in one object:

```{r, create_new_data}
new_data_raw <- cbind(data_raw[c(y_variable, x_variables_cont)], one_hot_data)
new_x_variables <- setdiff(names(new_data_raw), y_variable)
```

## Creating training and testing dataset

70/30 partition into training/testing datasets will be used.

```{r, data_partitioning}
set.seed(306068)
training_obs <- caret::createDataPartition(y = new_data_raw[[y_variable]], 
                                           p = 0.7, 
                                           list = FALSE)
data_train <- new_data_raw[training_obs,]
data_test <- new_data_raw[-training_obs,]
```

Additionally 5-time Cross validation will be used.

```{r, cv_params}
train_control_params <- trainControl(method = "cv", 
                                     number = 5, 
                                     # if classProbs = FALSE then caretEnsemble fails since not all model return the probabilities
                                     classProbs = TRUE)
```

## Variable transformations (part 2)

Normalize continuous variables (based on parameters form training data)

```{r, data_normalization}
means <- apply(X = data_train[, x_variables_cont], MARGIN = 2, FUN = mean)
sds <- apply(X = data_train[, x_variables_cont], MARGIN = 2, FUN = sd)
data_train[, x_variables_cont] <- lapply(X = x_variables_cont, FUN = function(i) (data_train[[i]] - means[i]) / sds[i])
data_test[, x_variables_cont] <- lapply(X = x_variables_cont, FUN = function(i) (data_test[[i]] - means[i]) / sds[i])
```

# Model training

## Decision tree

First model to train will be a **decision tree**. Apart from using default {caret} parameters I'll add hyperparametr: **cp**. It determines the complexity of a tree it impose restrictions on tree growth. 

```{r, train_dec_tree}
model_dec_tree <- caret::train(form = create_model_formula(y_variable, new_x_variables), 
                               data = data_train, 
                               method = "rpart", 
                               tuneGrid = data.frame(cp = seq(0, .05, .005)),
                               trControl = train_control_params)
```

### cp tuning {.tabset}

#### Table

```{r, dec_tree_hyper}
table_with_highlight(data_ = model_dec_tree$results, col_name = "Accuracy", function_to_use = which.max)
```

#### Plot

```{r, dec_tree_hyper_plot}
plot(model_dec_tree)
```

### {-}

Best value for **cp** is **0.005** which means that any split that does not decrease overall lack of fit by a factor of 0.005 is not attempted.

### Model results

Take a look at the splits of the tree.

```{r, dec_tree_splits}
rpart.plot::rpart.plot(model_dec_tree$finalModel)
```

It is a quite complicated tree. We can see that **marital.status2Married.civ.AF.spouse**, **capital.gain**, **capital.loss**, **educationBachelors**, **educationMasters**, **feat06** and **feat02** are used. How important are specific variables?

```{r, var_imp_dec_tree}
var_imp_plot(data_ = varImp(model_dec_tree$finalModel) %>% mutate(variable = row.names(.)), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - Decision Tree", n_limit = 10)
```

**capital.gain**, **marital.status2Married.civ.AF.spouse**, **relationshipHusband** and **occupation2Prof.specialty** are the most important variables.

## Random forest

Next model to use is random forest. I'll change number of trees (**ntree**) from 500 to 200. Additionally I'll try to find the optimal value of **mtry** parameter. In order to do so I'll search the values between 6 and 4 (the default value is square root from the number of predictors - in ths case $\sqrt{104} = 10.2$)

```{r, train_random_forest}
file_to_look_for <- "model_rf_class.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_random_forest <- caret::train(form = create_model_formula(y_variable, new_x_variables),
                                        data = data_train, 
                                        method = "rf",
                                        ntre = 200,
                                        tuneGrid = data.frame(mtry = seq(6, 14, by = 2)),
                                        trControl = train_control_params)
    print(Sys.time() - start) # 30 minutes
    saveRDS(object = model_random_forest, file = file_to_look_for)
} else {
    model_random_forest <- readRDS(file = file_to_look_for)
}
```

### mtry tuning {.tabset}

#### Table

```{r, rf_hyper}
table_with_highlight(model_random_forest$results, "Accuracy", which.max)
```

#### Plot

```{r, rf_hyper_plot}
plot(model_random_forest)
```

### {-}

Best value for **ntry** is **10** which means that for each of the trees only 10 randomly selected predictors will be used.

### Model results

Which features are the most important?

```{r, var_imp_rf}
var_imp_plot(data_ = varImp(model_random_forest)$importance %>% mutate(variable = row.names(.)), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - Random Forest", n_limit = 20)
```

**capital.gain**, **feat04**, **marital.status2Married.civ.AF.spouse**, **feat06** and **age** are the most important variables when I use Random Forest.

## Generalized Boosted Regression Modeling (GBM)

Next model is GBM. In this case 4 hyperparameters will be optimized:

- *ntree* - number of trees. Values from 100 to 300 (by 50) will be checked.
- *interaction.depth* - depth of a tree. Since there are 100 predictors, I'll check only values of 1 and 2.
- *shrinkage* - learning rate parameter. Only 2 values of this parameter will be used: 0.01 and 0.1
- *n.minobsinnode* - minimum number of observations in split. Since the training dataset is quite large I'll try values from 100 to 250 (by 50).

```{r, train_gbm}
file_to_look_for <- "model_gbm_class.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_gbm <- caret::train(form = create_model_formula(y_variable, new_x_variables),
                              data = data_train, 
                              method = "gbm",
                              trControl = train_control_params, 
                              tuneGrid = expand.grid(n.trees = seq(100, 300, by = 50),
                                                     interaction.depth = c(1, 2), 
                                                     shrinkage = c(0.01, 0.1), 
                                                     n.minobsinnode = seq(100, 250, 50)),
                              distribution = "bernoulli",
                              verbose = FALSE)
    print(Sys.time() - start) # 10 mins
    saveRDS(object = model_gbm, file = file_to_look_for)
} else {
    model_gbm <- readRDS(file = file_to_look_for)
}
```

### GBM tuning {.tabset}

#### Table

```{r, gbm_hyper}
table_with_highlight(model_gbm$results %>% top_n(n = 10, wt = Accuracy), "Accuracy", which.max)
```

#### Plot

```{r, gbm_hyper_plot}
plot(model_gbm)
```

As we can suspect the quality of the model gains as the number of trees increases (**ntree**). Model turned out to be the best for higher learning rate (**shrinkage** = 0.1) and depth of a tree (**interaction.depth** = 2). In terms of minimum observations in split the differences were marginal, but as suspected the lowest value (**n.minobsinnode** = 100) was the best one.

### Model results

What are the most important features?

```{r, var_imp_gbm}
var_imp_plot(data_ = summary.gbm(object = model_gbm$finalModel, plotit = FALSE), 
             var_name = "var", 
             var_imp_name = "rel.inf", plot_title = "Variable importance - GBM", n_limit = 20)
```

In GBM model the most important features were **marital.status2Married.civ.AF.spouse**, **capital.gain**, **feat04**, , **feat06**, **age** and **feat02**.

## eXtreme Gradient Boosting (XGBoost)

{caret} package enables to tune 7 parameters for XGBoost model. 3 of them will be search for the optimal values:

- *min_child_weight* - minimum number of observations in terminal node. Searching from 0.5% to 1% of number of observations (by 0.1%)
- *nrounds* - number of trees. Searching between 50 and 150 (by 25)
- *eta* - learning parameter. Searching between 0.05 to 0.4 (by 0.05)

Other 4 will be set constant:

- *colsample_bytree* - fraction of predictors to use (set to $\frac{\sqrt{104}}{104}$ ~ 10%)
- *subsample* - size (in %) of subsample drawn from training dataset (set to 80)
- *gamma* - Minimum Loss Reduction (set to 1)
- *max_depth* - maximum depth of tree (set to 8)

```{r, traing_xgboost}
tune_params_xgboost <- list(min_child_weight = nrow(data_train) * seq(.5, 1, .1) / 100,
                            nrounds = seq(50, 150, 25),
                            eta = seq(0.05, .4, by = .05))

static_params_xgboost <- list(colsample_bytree = sqrt(length(new_x_variables)) / length(new_x_variables),
                              subsample = 0.8, 
                              gamma = 1,
                              max_depth = 8)

file_to_look_for <- "model_xgboost_class.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_xgboost <- caret::train(form = create_model_formula(y_variable, new_x_variables),
                                  data = data_train,
                                  method = "xgbTree",
                                  trControl = train_control_params, 
                                  tuneGrid = do.call("expand.grid", c(static_params_xgboost, tune_params_xgboost)),
                                  verbosity = 0)
    print(Sys.time() - start) # 7 mins
    saveRDS(object = model_xgboost, file = file_to_look_for)
} else {
    model_xgboost <- readRDS(file = file_to_look_for)
}
```

### xGBoost tuning {.tabset}

#### Table

```{r, xgb_hyper}
table_with_highlight(model_xgboost$results %>% top_n(n = 10, wt = Accuracy), "Accuracy", which.max)
```

#### Plot

```{r, xgb_hyper_plot}
plot(model_xgboost)
```

#### Non-standard plot

```{r, xgb_hyper_plot_other}
do.call(rbind, args = lapply(X = names(tune_params_xgboost), FUN = function(i) {
    model_xgboost$results %>% select(tune_val = i, meas_val = Accuracy) %>% mutate(flag = i)
})) %>% 
    ggplot(mapping = aes(x = tune_val, y = meas_val)) + 
    geom_point() +
    facet_wrap(~flag, scales = "free_x") +
    geom_smooth(method = "loess", formula = "y ~ x") +
    labs(x = "Value of tuned parameter", y = "Accuracy", title = "Values of tuned parameters vs Accuracy") +
    theme_minimal()
```

As suspected larger **eta** and **nrounds** but lower **min_child_weight** lead to better results. But the best results were achieved with **eta** = 0.15, **nrounds** = 150 and **min_child_weight** = 148. The best **eta** point could be observed as an elbow point in both standard and non-standard plots.

### Model results

What are the most important features?

```{r, var_imp_xgb}
var_imp_plot(data_ = varImp(model_xgboost)$importance %>% mutate(variable = row.names(.)) %>% filter(Overall > 0), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - xGBoost", n_limit = 20)
```

In xGBoost model the most important features were **capital.gain**, **marital.status2Married.civ.AF.spouse**,  **feat04**, **age**, **feat02** and **feat06**. It is the same set of features as in GBM model (slightly different order).

## Logistic regression

As a benchmark model I'll use simple logistic regression model. 

```{r, train_log_reg}
model_logistic_regr <- caret::train(form = create_model_formula(y_variable, new_x_variables),
                                    data = data_train,
                                    method = "glm",
                                    trControl = train_control_params, 
                                    family = "binomial")
```

*Note* - it didn't converge because of colinearity of one-hot encoded variables that included all the levels.

# Ensembling and stacking

I'll use all the previous models for ensembling and stacking.

```{r, list_ensemble}
file_to_look_for <- "model_list_class.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_list <- caretEnsemble::caretList(create_model_formula(y_variable, new_x_variables),
                                           data = data_train,
                                           tuneList = list(
                                               rpart = caretModelSpec(method = "rpart", tuneGrid = model_dec_tree$bestTune),
                                               rf = caretModelSpec(method = "rf", ntree = 200, tuneGrid = model_random_forest$bestTune),
                                               glm = caretModelSpec(method = "glm", family = "binomial"),
                                               gbm = caretModelSpec(method = "gbm", distribution = "bernoulli", verbose = FALSE, tuneGrid = model_gbm$bestTune),
                                               xgbTree = caretModelSpec(method = "xgbTree", verbosity = 0, tuneGrid = model_xgboost$bestTune)
                                           ),
                                           trControl = train_control_params)
    
    print(Sys.time() - start) # 5 mins
    saveRDS(object = model_list, file = file_to_look_for)
} else {
    model_list <- readRDS(file = file_to_look_for)
}
```

What is the correlations between models?

## Correlation {.tabset}

### Plot

```{r, correlation_between_models_plot}
corrplot::corrplot(modelCor(resamples(model_list)), method = "number")
```

### Table

```{r, correlation_between_models_table}
modelCor(resamples(model_list)) %>% 
    as.data.frame() %>% 
    mutate(id = row.names(.)) %>% 
    pivot_longer(cols = setdiff(names(.), "id")) %>% 
    mutate(p1 = pmin(id, name), p2 = pmax(id, name)) %>% 
    select(p1, p2, value) %>% 
    unique() %>% 
    filter(p1 != p2) %>% 
    top_n(n = 4, wt = abs(value)) %>% 
    arrange(-value) %>% 
    kable(col.names = c("Model 1", "Model 2", "Correlation coeff.")) %>% 
    kable_styling()
```

## {-}

The highest correlation (in absolute terms) are between:

- GBM and Random Forest: 0.9699
- GBM and Logistic Regression: 0.9313
- Logistic Regression and Random Forest: 0.9031
- xGBoost and and Decision tree: -0.8201

Those findings are consistent with what was seen when analyzing each model separately. The negative/weak correlation between Decision Tree model and other model is expected based on the previous results.

## Ensembling

```{r, train_ensemble}
model_ensembled <- caretEnsemble::caretEnsemble(model_list)
summary(model_ensembled)
```

By default {caret} packages uses linear regression to determine the weights for each model. Lets look at variable importance:

```{r, var_imp_ensemble}
res_var_imp_ens <- varImp(model_ensembled) %>% 
    mutate(variable = rownames(.)) %>% 
    gather("model", "importance", -variable)

imp_lvls <- res_var_imp_ens %>% 
    filter(model == "overall") %>% top_n(n = 10, wt = importance) %>% arrange(importance) %>%  .$variable

res_var_imp_ens %>% 
    filter(variable %in% imp_lvls) %>% 
    mutate(variable2 = factor(x = variable, levels = imp_lvls)) %>%
    ggplot(mapping = aes(x = variable2, y = importance, color = model)) +
    geom_point(mapping = aes(shape = model == "overall", 
                             size = model == "overall")) +
    coord_flip() +
    theme_minimal() +
    scale_size_manual(guide = "none", values = c("TRUE" = 4, "FALSE" = 3)) +
    scale_color_viridis_d() +
    scale_shape_manual(values = c("TRUE" = 5, "FALSE" = 16)) +
    labs(x = "variables", color = "Model", shape = "Is it overall\nmodel?",
         title = "Variable importance from ensemble model", y = "Variable importance",
         subtitle = "top 10 variables from overall model") +
    theme(legend.position = "bottom")
```

## Stacking 

As stack model I'll-- use XGBoost and GLM with lasso regularization as top layer models for comparison.

```{r, train_model_stacked_xgb}
stacked_tr <- trainControl(method = "boot", 
                           number = 10,
                           savePredictions = "final",
)

file_to_look_for <- "model_stack_class_xgb.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_stacked_xgb <- caretEnsemble::caretStack(model_list,
                                                   method = "xgbTree",
                                                   trControl = stacked_tr)
    print(Sys.time() - start) # 4 mins
    saveRDS(object = model_stacked_xgb, file = file_to_look_for)
} else {
    model_stacked_xgb <- readRDS(file = file_to_look_for)
}
```

```{r, train_model_stacked_log}
model_stacked_glmnet <- caretEnsemble::caretStack(model_list,
                                                  method = "glmnet",
                                                  trControl = stacked_tr)
```

# Model assessment 

As an assessment methods I'll use Accuracy First, loo Comparison between training and testing data.

```{r, creating_comparison_data, warning=FALSE,message=FALSE}
model_list_specs <- list("Decision tree" = list(model_n = "model_dec_tree"),
                         "Random forest" = list(model_n = "model_random_forest"),
                         "GBM"           = list(model_n = "model_gbm"),
                         "XGBoost"       = list(model_n = "model_xgboost"),
                         "Logistic"      = list(model_n = "model_logistic_regr"),
                         "Ensembled"     = list(model_n = "model_ensembled"),
                         "Stacked (XGB)" = list(model_n = "model_stacked_xgb"),
                         "Stacked (GLM)"  = list(model_n = "model_stacked_glmnet")
                         )

assess_list <- list()
for (i in names(model_list_specs)) {
    this_model <- get(model_list_specs[[i]]$model_n)
    this_list <- list("train" = roc_me(data_train, y_variable, model = this_model),
                      "test"  = roc_me(data_test, y_variable, model = this_model)) 
    names(this_list) <- paste0(i, ": ", names(this_list))
    assess_list <- c(assess_list, this_list)
}
# original ROC data could consist thousands of points. for plotting I'll simplify those
simplified_roc_data <- simplify_roc_data(model_list = assess_list)
simplified_roc_data <- simplified_roc_data %>% 
    mutate(model_type = factor(model_type, levels = names(model_list_specs)))
```

## Comparison {.tabset}

### By dataset

```{r, comparison_by_dataset}
plot_model_comparison(roc_data = simplified_roc_data, color_by = "model_type") +
    facet_wrap(~data_type) +
    labs(title = "Comparing ROC curves", subtitle = "data type", color = "Model")
```

### By model

```{r, comparison_by_model}
plot_model_comparison(roc_data = simplified_roc_data, color_by = "data_type") +
    facet_wrap(~model_type, nrow = 2) +
    labs(title = "Comparing ROC curves", subtitle = "model type", color = "Data")
```

### Table

```{r, comparison_table}
summ_up_table <- get_metrics(assess_list) %>% 
    select(model_type, auc, gini, data_type) %>% 
    pivot_wider(names_from = c("data_type"), values_from = c("auc", "gini")) %>% 
    arrange(abs(auc_train - auc_test))

saveRDS(object = summ_up_table, file = "class_summ_up_table.rds")

summ_up_table %>% 
    mutate(rank_train = rank(-auc_train), rank_test = rank(-auc_test)) %>% 
    mutate(auc_train = scales::percent(auc_train),
           auc_test = scales::percent(auc_test),
           gini_train = scales::percent(gini_train),
           gini_test = scales::percent(gini_test)) %>% 
    kable(booktabs = T, col.names = c("Model", "AUC (Train)", "AUC (Test)", "Gini (Train)", "Gini (Test)", "Rank (Train)", "Rank (Test)")) %>%
    kable_styling() %>%
    row_spec(which(summ_up_table$auc_train == max(summ_up_table$auc_train)), bold = T, color = "white", background = "green") %>% 
    row_spec(which(summ_up_table$auc_test == max(summ_up_table$auc_test)), underline = T)
```

## Results

- All of the models seems overfitted, although smallest difference (in absolute terms) was in GBM model 
- Random forest had highest AUC for train, Stacked XGB for test
- Stacked XGBoost yield best results overall

# Summary and conclusions

- Stacked with XGBoost as top layer model was the best approach
- Stacking with logistic regression yields same (ish) results as ensembling
- being married and living with a spouse and capital gain where the most important features
