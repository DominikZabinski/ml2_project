---
title: "Salary Prediction Classification"
subtitle: "Classification on Salary whether less than 50K or greater than 50K"
output: 
    html_document:
        toc: true
        toc_float: true
        smooth_scroll: true
---

```{r setup, include=FALSE}
# libraries
library(tidyverse) # data wrangling
library(corrplot) # nice correlation matrix charts
library(kableExtra) # styling tables
library(caret) # ease the process of training/testing different models 
library(caretEnsemble) # ensembling and stacking
# loading specific libraries
library(rpart)
library(randomForest)
library(gbm)
library(xgboost)
library(pROC)

knitr::opts_chunk$set(cache = FALSE)
options(scipen = 999)

#' Describe single continuous variable from the dataset
#'
#' @param data_ 
#' @param variable 
#'
#' @return
#' @export
#'
#' @examples
describe_cont_var <- function(data_, variable) {
    data.frame(name = variable, 
               min = min(data_raw[[variable]], na.rm = T),
               mean = mean(data_raw[[variable]], na.rm = T),
               sd = sd(data_raw[[variable]], na.rm = T),
               max = max(data_raw[[variable]], na.rm = T),
               missing = sum(is.na(data_raw[[variable]])),
               distinct = dplyr::n_distinct(data_raw[[variable]]))
}

#' Describe single discrete variable from the dataset
#'
#' @param data_ 
#' @param variable 
#'
#' @return
#' @export
#'
#' @examples
describe_disc_var <- function(data_, variable) {
    res <- table(data_[[variable]]) / nrow(data_)
    res2 <- table(data_[[variable]])
    order_vals <- order(res, decreasing = T)
    to_display <- order_vals[1:min(3, length(order_vals))]
    data.frame(name = variable,
               n_levels = length(res),
               missing = sum(is.na(data_[[variable]])),
               top_levels = paste0(names(res)[to_display], " (", round(100 * res[to_display], 2), ")", collapse = "; "),
               dist_range = paste0(round(100 * min(res), 2), "% - ", round(100 * max(res), 2), "%"),
               n_range = paste0(min(res2), " - ", max(res2))
               )
}

#' Create formula for the model
#'
#' @param y 
#' @param x 
#'
#' @return
#' @export
#'
#' @examples
create_model_formula <- function(y, x) {
   paste0(c(y, paste0(x, collapse = " + ")), collapse = "~") %>% 
        as.formula()    
}

#' Predicting probability for success class
#'
#' @param data_ dataset
#' @param success_class character with name of success class 
#' @param model object with model
#'
#' @return
#' @export
#'
#' @examples
pred_success_prob <- function(data_, success_class = "morethan50K", model) {
    if (is(model, "caretStack") || is(model, "glm")) {
        predict(model, newdata = data_, type = "prob")
    } else {
        predict(model, newdata = data_, type = "prob")[, success_class]
    }
}

#' Creating roc object 
#'
#' @param data_ dataset
#' @param dependent string with the name of dependent variable
#' @param success_class character with name of success class 
#' @param model obejct with model
#'
#' @return
#' @export
#'
#' @examples
roc_me <- function(data_, dependent, success_class = "morethan50K", model) {
    pROC::roc(data_[[dependent]], pred_success_prob(data_, success_class, model))
}


#' Simplifies data.frame with specifity and sensitivity values
#'
#' @param roc_curve_data 
#' @param thres_points 
#'
#' @return
#' @export
#'
#' @examples
simplify_roc_curve <- function(roc_curve_data, thres_points = 100) {
    res <- data.frame(sensitivity = roc_curve_data$sensitivities, 
                      specifity = roc_curve_data$specificities)
    if (nrow(res) > thres_points) {
        res <- res %>% 
            mutate(specifity = round(specifity, 2)) %>% 
            group_by(specifity) %>% 
            summarise(mini = min(sensitivity), maxi = max(sensitivity)) %>% 
            pivot_longer(cols = c("mini", "maxi"), values_to = "sensitivity") %>% 
            select(specifity, sensitivity) %>% 
            unique() %>% 
            arrange(specifity, sensitivity)
    }
    return(res)
}

#' Simplifies data.frames with specifity and sensitivity values of model list
#'
#' @param model_list list of model objects
#'
#' @return
#' @export
#'
#' @examples
simplify_roc_data <- function(model_list) {
    do.call(what = "rbind", 
            args = lapply(X = names(model_list), FUN = function(x) {
                res <- simplify_roc_curve(model_list[[x]])
                res$model <- x
                res$model_type <- substr(x, 1, regexpr(pattern = ":", text = x, fixed = T) - 1)
                res$data_type <- ifelse(regexpr(pattern = "test", text = x) > 0, "test", "train")
                res$data_type <- factor(res$data_type, levels = c("train", "test"))
                res
            }))
}

#' Get AUC and gini from model
#'
#' @param model_list list with model objects
#'
#' @return
#' @export
#'
#' @examples
get_metrics <- function(model_list) {
    do.call(what = "rbind", 
            args = lapply(X = names(model_list), FUN = function(x) {
                res_auc <- pROC::auc(model_list[[x]])
                res <- data.frame(auc = res_auc, gini = 2 * res_auc - 1, model = x, 
                           model_type = substr(x, 1, regexpr(pattern = ":", text = x, fixed = T) - 1),
                           data_type = ifelse(regexpr(pattern = "test", text = x) > 0, "test", "train")
                )
                res$data_type <- factor(res$data_type, levels = c("train", "test"))
                res
            }))
}

#' Creating ROC plot
#'
#' @param roc_data data.frame with sensitivity and specifity
#'
#' @return
#' @export
#'
#' @examples
plot_model_comparison <- function(roc_data, color_by) {
    roc_data %>%
        ggplot() +
        geom_line(mapping = aes(x = specifity, y = sensitivity, color = .data[[color_by]])) +
        geom_segment(mapping = aes(x = 1, xend = 0, y = 0, yend = 1), 
                     color = "grey", 
                     linetype = "dashed") +
        scale_x_reverse() +
        theme_minimal() + 
        theme(panel.grid.minor = element_blank(), legend.position = "bottom") +
        coord_fixed() +
        scale_color_viridis_d(option = "D")
}

```

```{r, reading_data}
data_raw <- read_csv(file = "c3.csv", show_col_types = FALSE)
```

# Description of data and problem
## Data
1994 Census database. Quick look at the dataset structure.

```{r, glimpse_at_data}
glimpse(data_raw)
```

42,651 observations, 26 variables.

## Problem
Predict whether a person makes over 50,000 a year.

# Descriptive analyses

Establish dependent variable (**y_variable**) and predictors (**x_variables**).

```{r, data_cleaning}
# Change the column names to avoid possible problems in the future
names(data_raw) <- make.names(names(data_raw))
# 'salary' as dependent variable
y_variable <- "salary"
# rest (excluding 'id') are predictors
x_variables <- setdiff(names(data_raw), c("id", "salary"))
```

Quick look at the dependent variable:

```{r, y_distribution}
data_raw %>% 
    group_by_at(y_variable) %>% 
    summarise(count_n = n()) %>% 
    mutate(m = paste0(.data[[y_variable]], " (", round(100 * count_n / sum(count_n), 1), "%)")) %>% 
    ggplot(mapping = aes(x = 1)) + 
    geom_col(mapping = aes(fill = m, y = count_n), 
             position = "stack", color = "white", size = 2) +
    coord_polar(theta = "y") +
    labs(fill = y_variable, 
         title = "Distribution of dependent variable") +
    theme_minimal() +
    scale_fill_viridis_d() +
    theme(panel.grid = element_blank(), 
          axis.text = element_blank(), 
          axis.title = element_blank())
```

Ii is not balanced but no need for over/under sampling.

Levels contains special characters - remove them (it will cause problems when trying to get predicted probabilities).

```{r, y_variable_transform}
y_variable_values_adj <- rep("atmost50K", times = nrow(data_raw))
y_variable_values_adj[data_raw[[y_variable]] == ">50K"] <- "morethan50K"
data_raw[[y_variable]] <- y_variable_values_adj
```

Spit variable into continuous and discrete variables:

```{r, split_vars}
x_variables_cont <- x_variables[sapply(X = x_variables, FUN = function(x) is(data_raw[[x]], "numeric"))]
x_variables_disc <- x_variables[!sapply(X = x_variables, FUN = function(x) is(data_raw[[x]], "numeric"))]
```

Take a look at continuous variables:

```{r, descr_cont}
table_descr_cont <- do.call(what = rbind, args = lapply(X = x_variables_cont, FUN = function(i) describe_cont_var(data_raw, i)))

saveRDS(object = table_descr_cont, file = "class_table_descr_cont.rds")

table_descr_cont %>% 
    mutate(min = round(min, 2), mean = round(mean, 2), sd = round(sd, 2), max = round(max, 2)) %>% 
    kable(booktabs = T) %>%
    kable_styling()
```

- no need for imputation (no missing records)
- **education.num** has 16 distinct values - might be good idea to change it to discrete

Discrete variables

```{r, descr_disc}
table_descr_disc <- do.call(rbind, lapply(X = x_variables_disc, FUN = function(i) describe_disc_var(data_raw, i)))

saveRDS(object = table_descr_disc, file = "class_table_descr_disc.rds")

table_descr_disc %>% 
    kable(booktabs = T) %>%
    kable_styling()
```

- **education** has 16 levels - same as **education.num** - possible duplicate of information
- **marital.status**, **native.country**, **occupation** and **workclass* has levels with less than 30 observations - those might cause problems with resampling

Examine further **education** and **education.num** variables

```{r, check_education}
data_raw %>% 
    group_by(education, education.num) %>% 
    summarise(count = n()) %>% 
    arrange(education.num)
```

Loose the **education.num** variable

```{r, remove_education}
x_variables_cont <- setdiff(x_variables_cont, "education.num")
```

Examine variables with low groups - marital.status

```{r, marital_exam}
marital_sum <- data_raw %>% 
    group_by_at("marital.status") %>% 
    summarise(count_n = n(), dist = sum(salary == "morethan50K") / count_n)
marital_sum %>% 
  kable() %>%
  kable_styling() %>%
  row_spec(which(marital_sum$count_n < 30), bold = T, color = "white", background = "red")
```

Since Married-AF-spouse has similar percentage of observations earning > 50K we join those two together

```{r, marital_transform}
data_raw <- data_raw %>% 
    mutate(marital.status2 = case_when(marital.status %in% c("Married-AF-spouse", "Married-civ-spouse") ~ "Married-civ-AF-spouse",
                                      .default = marital.status))
x_variables_disc <- c(setdiff(x_variables_disc, "marital.status"), "marital.status2")
```

What about **native.country**?

```{r, native_exam}
native_sum <- data_raw %>% 
    group_by_at("native.country") %>% 
    summarise(count_n = n(), dist = sum(salary == "morethan50K") / count_n)
native_sum %>% 
  kable() %>%
  kable_styling() %>%
  row_spec(which(native_sum$count_n < 30), bold = T, color = "white", background = "red")
```

There are some low levels:

- Cambodia
- Holand-Netherlands
- Honduras
- Hong
- Hungary
- Laos
- Outlying-US(Guam-USVI-etc)
- Scotland
- Thailand
- Trinidad&Tobago
- Yugoslavia

```{r, native_dendr}
native_cluster <- as.data.frame(native_sum)
native_suff <- substr(native_cluster$native.country, 1, 4)
native_low_level <- native_cluster %>% 
    filter(count_n < 30) %>% .$native.country
native_low_level_idx <- which(native_cluster$native.country %in% native_low_level)
native_suff[native_low_level_idx] <- paste0("**", native_suff[native_low_level_idx], "**")
row.names(native_cluster) <- paste(native_suff, 
                                   native_cluster$count_n, 
                                   round(100 * native_cluster$dist, 1), sep = "-")
par(cex = .6)
native_cluster %>% 
    select(dist) %>% 
    dist() %>% 
    hclust(., method = "average") %>% 
    plot()
par(cex = 1)
```

Not very helpful, resorting to experts knowledge

```{r, native_transform}
data_raw <- data_raw %>% 
    mutate(native.country2 = case_when(native.country %in% c("Thailand", "Laos") ~ "Thailand_Laos",
                                       native.country %in% c("Cambodia", "Philippines") ~ "Cambodia_Philippines",
                                       native.country %in% c("Honduras", "Jamaica") ~ "Honduras_Jamaica",
                                       native.country %in% c("Trinadad&Tobago", "Haiti") ~ "TT_Haiti",
                                       native.country %in% c("Hungary", "Poland", "Yugoslavia") ~ "Hungary_Poland_Yugoslavia",
                                       native.country %in% c("Scotland", "England") ~ "Brits",
                                       native.country %in% c("?", "Holand-Netherlands", "Outlying-US(Guam-USVI-etc)") ~ "Other",
                                       .default = native.country))
x_variables_disc <- c(setdiff(x_variables_disc, "native.country"), "native.country2")
```

Occupation

```{r, occupation_exam}
occupation_sum <- data_raw %>% 
    group_by_at("occupation") %>% 
    summarise(count_n = n(), dist = sum(salary == "morethan50K") / count_n)
occupation_sum %>% 
  kable() %>%
  kable_styling() %>%
  row_spec(which(occupation_sum$count_n < 30), bold = T, color = "white", background = "red")
```

Join **Armed-Forces** with **Protective-serv** and **?** with **Other-service**

```{r, occupation_transform}
data_raw <- data_raw %>% 
    mutate(occupation2 = case_when(occupation %in% c("Armed-Forces", "Protective-serv") ~ "Protection",
                                   occupation %in% c("?", "Other-service") ~ "Other",
                                   .default = occupation))
x_variables_disc <- c(setdiff(x_variables_disc, "occupation"), "occupation2")
```

**workclass**

```{r, workclass_exam}
workclass_sum <- data_raw %>% 
    group_by_at("workclass") %>% 
    summarise(count_n = n(), dist = sum(salary == "morethan50K") / count_n)
workclass_sum %>% 
  kable() %>%
  kable_styling() %>%
  row_spec(which(workclass_sum$count_n < 30), bold = T, color = "white", background = "red")
```

Join **Never-worked** and **Without-pay**

```{r, workclass_transform}
data_raw <- data_raw %>% 
    mutate(workclass2 = case_when(workclass %in% c("Never-worked", "Without-pay") ~ "NoIncome",
                                   .default = workclass))
x_variables_disc <- c(setdiff(x_variables_disc, "workclass"), "workclass2")
```

Just to make sure all the transformation went as expected.

```{r, descr_disc_after}
do.call(rbind, lapply(X = x_variables_disc, FUN = function(i) describe_disc_var(data_raw, i)))
```

Still 2 variables has < 30 in some levels but we are OK with it.

Is there a correlation between continuous variables?

```{r, corrplot}
corrplot::corrplot(corr = cor(data_raw[, x_variables_cont]), method = "color")
```

No clear correlation between continuous variables.

# Data preparation

## Variable transformations

Perform one-hot encoding for discrete variables:

```{r, one_hot}
one_hot_data <- data.frame(predict(object = dummyVars(formula = " ~ .", data = data_raw[x_variables_disc]), 
                                   newdata = data_raw[x_variables_disc]))
```

Bind all in one object:

```{r, create_new_data}
new_data_raw <- cbind(data_raw[c(y_variable, x_variables_cont)], one_hot_data)
new_x_variables <- setdiff(names(new_data_raw), y_variable)
```

## Creating training and testing dataset

70/30 partition into training/testing datasets will be used.

```{r, data_partitioning}
set.seed(306068)
training_obs <- caret::createDataPartition(y = new_data_raw[[y_variable]], 
                                           p = 0.7, 
                                           list = FALSE)
data_train <- new_data_raw[training_obs,]
data_test <- new_data_raw[-training_obs,]
```

Additionally 5-time Cross validation will be used.

```{r, cv_params}
train_control_params <- trainControl(method = "cv", 
                                     number = 5, 
                                     # if classProbs = FALSE then caretEnsemble fails since not all model return the probabilities
                                     classProbs = TRUE)
```

## Variable transformations (part 2)

Normalize continuous variables (based on parameters form training data)

```{r, data_normalization}
means <- apply(X = data_train[, x_variables_cont], MARGIN = 2, FUN = mean)
sds <- apply(X = data_train[, x_variables_cont], MARGIN = 2, FUN = sd)
data_train[, x_variables_cont] <- lapply(X = x_variables_cont, FUN = function(i) (data_train[[i]] - means[i]) / sds[i])
data_test[, x_variables_cont] <- lapply(X = x_variables_cont, FUN = function(i) (data_test[[i]] - means[i]) / sds[i])
```

# Model training

## Decision tree

Decision tree with default parameters

```{r, train_dec_tree}
model_dec_tree <- caret::train(form = create_model_formula(y_variable, new_x_variables), 
                               data = data_train, 
                               method = "rpart", 
                               trControl = train_control_params)
```

Take a look at the splits:

```{r, dec_tree_splits}
rpart.plot::rpart.plot(model_dec_tree$finalModel)
```

How important are the variables?

```{r, var_imp_dec_tree}
varImp(model_dec_tree$finalModel) %>% 
    filter(., Overall > 0) %>% 
    ggplot(mapping = aes(x = row.names(.), y = Overall)) +
    geom_col() +
    coord_flip() +
    theme_minimal() +
    labs(x = "variables")
```

## Random forest

Random forest with default settings:

```{r, train_random_forest}
file_to_look_for <- "model_rf_class.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_random_forest <- caret::train(form = create_model_formula(y_variable, new_x_variables),
                                        data = data_train, 
                                        method = "rf",
                                        trControl = train_control_params)
    print(Sys.time() - start) # 25 minutes
    saveRDS(object = model_random_forest, file = file_to_look_for)
} else {
    model_random_forest <- readRDS(file = file_to_look_for)
}
```

Which features are the most important?

```{r, var_imp_rf}
varImp(model_random_forest)$importance %>%  
    filter(., Overall > 0) %>% 
    arrange(Overall) %>% 
    top_n(n = 20) %>% 
    ggplot(mapping = aes(x = row.names(.), y = Overall)) +
    geom_col() +
    coord_flip() +
    theme_minimal() +
    labs(x = "variables")
```

## Generalized Boosted Regression Modeling (GBM)

GBM model with default settings:

```{r, train_gbm}
file_to_look_for <- "model_gbm_class.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_gbm <- caret::train(form = create_model_formula(y_variable, new_x_variables),
                              data = data_train, 
                              method = "gbm",
                              trControl = train_control_params, 
                              distribution = "bernoulli",
                              verbose = FALSE)
    print(Sys.time() - start) #1.4 mins
    saveRDS(object = model_gbm, file = file_to_look_for)
} else {
    model_gbm <- readRDS(file = file_to_look_for)
}
```

What are the most important features?

```{r, var_imp_gbm}
summary.gbm(object = model_gbm$finalModel, plotit = FALSE) %>% 
    filter(., rel.inf > 0) %>% 
    arrange(rel.inf) %>% 
    top_n(n = 20) %>% 
    ggplot(mapping = aes(x = var, y = rel.inf)) +
    geom_col() +
    coord_flip() +
    labs(x = "variables")
```

## eXtreme Gradient Boosting (XGBoost)

XGBoost with default settings:

```{r, traing_xgboost}
file_to_look_for <- "model_xgboost_class.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_xgboost <- caret::train(form = create_model_formula(y_variable, new_x_variables),
                                  data = data_train,
                                  method = "xgbTree",
                                  trControl = train_control_params, 
                                  verbosity = 0)
    print(Sys.time() - start) # 11 mins
    saveRDS(object = model_xgboost, file = file_to_look_for)
} else {
    model_xgboost <- readRDS(file = file_to_look_for)
}
```

Variable importance

```{r, var_imp_xgb}
varImp(model_xgboost)$importance %>%  
    filter(., Overall > 0) %>% 
    top_n(n = 20) %>% 
    ggplot(mapping = aes(x = row.names(.), y = Overall)) +
    geom_col() +
    coord_flip() +
    theme_minimal() +
    labs(x = "variables")
```

## Logistic regression

As a benchmark model

```{r, train_log_reg}
model_logistic_regr <- caret::train(form = create_model_formula(y_variable, new_x_variables),
                                    data = data_train,
                                    method = "glm",
                                    trControl = train_control_params, 
                                    family = "binomial")
# Note - didnt converge because of colinearity of one-hot encoded variables that included all the levels
```

# Ensembling and stacking

We'll use all the previous models for ensembling.

```{r, list_ensemble}
file_to_look_for <- "model_list_class.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_list <- caretEnsemble::caretList(create_model_formula(y_variable, new_x_variables),
                                           data = data_train,
                                           methodList = c("rpart", "rf"),
                                           tuneList = list(
                                               glm = caretModelSpec(method = "glm", family = "binomial"),
                                               gbm = caretModelSpec(method = "gbm", distribution = "bernoulli", verbose = FALSE),
                                               xgbTree = caretModelSpec(method = "xgbTree", verbosity = 0)
                                           ),
                                           trControl = train_control_params)
    
    print(Sys.time() - start) #~1 hour
    saveRDS(object = model_list, file = file_to_look_for)
} else {
    model_list <- readRDS(file = file_to_look_for)
}
```

```{r, correlation_between_models}
caret::modelCor(caret::resamples(model_list))
```

## Ensembling

```{r, train_ensemble}
model_ensembled <- caretEnsemble::caretEnsemble(model_list)
summary(model_ensembled)
```

```{r, var_imp_ensemble}
res_var_imp_ens <- varImp(model_ensembled) %>% 
    mutate(variable = rownames(.)) %>% 
    gather("model", "importance", -variable)

imp_lvls <- res_var_imp_ens %>% 
    filter(model == "overall") %>% top_n(n = 20, wt = importance) %>% .$variable

res_var_imp_ens %>% 
    filter(variable %in% imp_lvls) %>% 
    mutate(variable2 = factor(x = variable, levels = imp_lvls)) %>%
    ggplot(mapping = aes(x = variable2, y = importance, color = model)) +
    geom_point(aes(shape = model == "overall", 
                   alpha = model == "overall",
                   size = model == "overall")) +
    coord_flip() +
    theme_minimal() +
    scale_alpha_manual(guide = "none", values = c("TRUE" = 1, "FALSE" = .7)) +
    scale_size_manual(guide = "none", values = c("TRUE" = 3, "FALSE" = 2)) +
    scale_color_viridis_d() +
    scale_shape_manual(values = c("TRUE" = 5, "FALSE" = 16)) +
    labs(x = "variables", color = "Model", shape = "Is it overall\nmodel?",
         title = "Variable importance from ensemble model",
         subtitle = "top 20")
```

## Stacking 

As stack model we use XGBoost and logistic regression for comparison.

```{r, train_model_stacked_xgb}
stacked_tr <- trainControl(method = "boot", 
                           number = 10,
                           savePredictions = "final",
)

file_to_look_for <- "model_stack_class_xgb.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_stacked_xgb <- caretEnsemble::caretStack(model_list,
                                                   method = "xgbTree",
                                                   trControl = stacked_tr)
    print(Sys.time() - start) # 4 mins
    saveRDS(object = model_stacked_xgb, file = file_to_look_for)
} else {
    model_stacked_xgb <- readRDS(file = file_to_look_for)
}
```

```{r, train_model_stacked_log}
model_stacked_log <- caretEnsemble::caretStack(model_list,
                                               method = "glm",
                                               trControl = stacked_tr)
```

What are the results?

```{r, summ}
varImp(model_stacked_xgb$ens_model)
```

```{r, summ_stack_log}
summary(model_stacked_log)
```

# Model assessment 

Comparison between training and testing data

```{r, creating_comparison_data, warning=FALSE,message=FALSE}
model_list_specs <- list("Decision tree" = list(model_n = "model_dec_tree"),
                         "Random forest" = list(model_n = "model_random_forest"),
                         "GBM"           = list(model_n = "model_gbm"),
                         "XGBoost"       = list(model_n = "model_xgboost"),
                         "Logistic"      = list(model_n = "model_logistic_regr"),
                         "Ensembled"     = list(model_n = "model_ensembled"),
                         "Stacked (XGB)" = list(model_n = "model_stacked_xgb"),
                         "Stacked (Log)"  = list(model_n = "model_stacked_log")
                         )

assess_list <- list()
for (i in names(model_list_specs)) {
    print(i)
    this_model <- get(model_list_specs[[i]]$model_n)
    this_list <- list("train" = roc_me(data_train, y_variable, model = this_model),
                      "test"  = roc_me(data_test, y_variable, model = this_model)) 
    names(this_list) <- paste0(i, ": ", names(this_list))
    assess_list <- c(assess_list, this_list)
}
simplified_roc_data <- simplify_roc_data(model_list = assess_list)
simplified_roc_data <- simplified_roc_data %>% 
    mutate(model_type = factor(model_type, levels = names(model_list_specs)))
```

```{r, comparison_by_dataset}
plot_model_comparison(roc_data = simplified_roc_data, color_by = "model_type") +
    facet_wrap(~data_type) +
    labs(title = "Comparing ROC curves", subtitle = "data type", color = "Model")
```

```{r, comparison_by_model}
plot_model_comparison(roc_data = simplified_roc_data, color_by = "data_type") +
    facet_wrap(~model_type, nrow = 2) +
    labs(title = "Comparing ROC curves", subtitle = "model type", color = "Data")
```

In the form of table:

```{r, comparison_table}
summ_up_table <- get_metrics(assess_list) %>% 
    select(model_type, auc, gini, data_type) %>% 
    pivot_wider(names_from = c("data_type"), values_from = c("auc", "gini")) %>% 
    arrange(abs(auc_train - auc_test))

saveRDS(object = summ_up_table, file = "class_summ_up_table.rds")

summ_up_table %>% 
    mutate(auc_train = scales::percent(auc_train),
           auc_test = scales::percent(auc_test),
           gini_train = scales::percent(gini_train),
           gini_test = scales::percent(gini_test)) %>% 
    kable(booktabs = T) %>%
    kable_styling() %>%
    row_spec(which(summ_up_table$auc_train == max(summ_up_table$auc_train)), bold = T, color = "white", background = "green") %>% 
    row_spec(which(summ_up_table$auc_test == max(summ_up_table$auc_test)), underline = T)
```

# Results

- All of the models seems overfitted, although smallest difference (in absolute terms) was in GBM model 
- Random forest had highest AUC for train, Stacked XGB for test
- Stacked XGBoost yield best results overall

# Summary and conclusions

- Stacked with XGBoost as top layer model was the best approach
- Stacking with logistic regression yields same (ish) results as ensembling
- being married and living with a spouse and capital gain where the most important features
