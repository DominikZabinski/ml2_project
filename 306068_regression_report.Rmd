---
title: "Red Wine Quality"
subtitle: "Predicting using regression methods"
output: 
    html_document:
        toc: true
        toc_float: true
        smooth_scroll: true
---

```{r setup, include=FALSE}
# libraries
library(tidyverse) 
library(corrplot) 
library(kableExtra) 
library(caret) 
library(caretEnsemble) 
library(rpart)
library(randomForest)
library(gbm)
library(xgboost)

# settings
knitr::opts_chunk$set(cache = FALSE)
options(scipen = 999)

# functions
source("__functions.R")
```

```{r, reading_data}
data_raw <- read_csv(file = "r2.csv", show_col_types = FALSE)
```

# Description of data and problem
## Data
Vinho Verde data. Quick look at the dataset structure.

```{r, glimpse_at_data}
glimpse(data_raw)
```

## Problem
Predict wine quality

# Descriptive analyses

Establish dependent variable (**y_variable**) and predictors (**x_variables**).

```{r, data_cleaning}
# Change the column names to avoid possible problems in the future
names(data_raw) <- make.names(names(data_raw))
# 'quality' as dependent variable
y_variable <- "quality"
# rest (excluding 'id') are predictors
x_variables <- setdiff(names(data_raw), c("id", y_variable))
```

Quick look at the dependent variable:

```{r, y_distribution}
ggplot(data = data_raw, mapping = aes(x = .data[[y_variable]])) + 
    geom_histogram(mapping = aes(y = after_stat(density)), binwidth = .5) +
    geom_density(alpha = .4, fill = "darkblue", bw = 2.5) +
    theme_minimal() +
    scale_x_continuous(breaks = floor(min(data_raw$quality)):ceiling(max(data_raw$quality))) +
    labs(title = "Distribution of wine quality", x = "Quality", y = "Density", subtitle = "bins: observed, line: estimated")
```

Take a look at continuous variables:

```{r, descr_cont}
table_descr_cont <- do.call(what = rbind, args = lapply(X = x_variables, FUN = function(i) describe_cont_var(data_raw, i)))

saveRDS(object = table_descr_cont, file = "regr_table_descr_cont.rds")

table_descr_cont %>% 
    mutate(min = round(min, 2), mean = round(mean, 2), sd = round(sd, 2), max = round(max, 2)) %>% 
    kable(booktabs = T) %>%
    kable_styling()
```

- no need for imputation (no missing records)
- minimum number of distinct values is 56 - no clear indication of turning variable into discret one.

Is there a correlation between variables?

```{r, corrplot}
corrplot::corrplot(corr = cor(data_raw[, c(x_variables, y_variable)]), method = "color")
```

- high correlation between *pH* and *fixed.acidicity*
- weak correlation between *citric.acid* and *violatile.acidicity*, *pH*
- no clear correlation between outcome variable and predictors except for *feat04* and *feat06*

# Data preparation 

## Creating training and testing dataset

70/30 partition into training/testing datasets will be used.

```{r, data_partitioning}
set.seed(306068)
training_obs <- caret::createDataPartition(data_raw$quality, 
                                           p = 0.7, 
                                           list = FALSE)
data_train <- data_raw[training_obs,]
data_test <- data_raw[-training_obs,]
```

Additionally 5-time Cross validation will be used.

```{r, cv_params}
train_control_params <- trainControl(method = "cv", 
                                     number = 5,
                                     savePredictions = "all")
```

## Variable transformations

Normalize continuous variables (based on parameters form training data)

```{r, data_normalization}
means <- apply(X = data_train[, x_variables], MARGIN = 2, FUN = mean)
sds <- apply(X = data_train[, x_variables], MARGIN = 2, FUN = sd)
data_train[, x_variables] <- lapply(X = x_variables, FUN = function(i) (data_train[[i]] - means[i]) / sds[i])
data_test[, x_variables] <- lapply(X = x_variables, FUN = function(i) (data_test[[i]] - means[i]) / sds[i])
```

# Model training

## Decision tree

First model to train will be a **decision tree**. Apart from using default {caret} parameters I'll add hyperparametr: **cp**. It determines the complexity of a tree it impose restrictions on tree growth. 

```{r, train_dec_tree}
model_dec_tree <- caret::train(form = create_model_formula("quality", x_variables), 
                               data = data_train, 
                               method = "rpart",
                               trControl = train_control_params, 
                               tuneGrid = data.frame(cp = seq(0, .05, .005))
)
```

What is the best value of **cp** parameter?

### cp tuning {.tabset}

#### Table

```{r, dec_tree_hyper}
table_with_highlight(data_ = model_dec_tree$results, col_name = "RMSE")
```

#### Plot

```{r, dec_tree_hyper_plot}
plot(model_dec_tree)
```

### {-}

Best value for **cp** is **0.025** which means that any split that does not decrease overall lack of fit by a factor of 0.025 is not attempted.

### Model results

Take a look at the splits of the tree.

```{r, dec_tree_splits}
rpart.plot::rpart.plot(model_dec_tree$finalModel)
```

It is a simple tree: first split is done according to the **alcohol** value, in next either **feat07** or **volatile.acidity** are used. Beside the split we should look into variable importance:

```{r, var_imp_dec_tree}
var_imp_plot(data_ = varImp(model_dec_tree$finalModel) %>% mutate(variable = row.names(.)) %>% filter(Overall > 0), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - Decision Tree")
```

**volatile.acidity**, **sulphates** and **citric.acid** seems to be the most important variables.

## Random forest

Next model to use is random forest. I'll change number of trees (**ntree**) from 500 to 200. Additionally I'll try to find the optimal value of **mtry** parameter. In order to do so I'll search the values between 4 and 10 (the default value is square root from the number of predictors - in ths case $\sqrt{21} = 4.5$)

```{r, train_random_forest}
model_random_forest <- caret::train(form = create_model_formula("quality", x_variables),
                                    data = data_train, 
                                    method = "rf", 
                                    ntre = 200,
                                    trControl = train_control_params, 
                                    tuneGrid = data.frame(mtry = 4:10))
```

### mtry tuning {.tabset}

#### Table

```{r, rf_hyper}
table_with_highlight(model_random_forest$results, "RMSE")
```

#### Plot

```{r, rf_hyper_plot}
plot(model_random_forest)
```

### {-}

Best value for **ntry** is **9** which means that for each of the trees only 9 randomly selected predictors will be used.

### Model results

Which features are the most important?

```{r, var_imp_rf}
var_imp_plot(data_ = varImp(model_random_forest)$importance %>% mutate(variable = row.names(.)) %>% filter(Overall > 0), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - Random Forest")
```

**alcohol**, **feat07** and **volatile.acidity** are the most important variables.

## Generalized Boosted Regression Modeling (GBM)

Next model is GBM. In this case 4 hyperparameters will be optimized:

- *ntree* - number of trees. Values from 100 to 300 (by 50) will be checked.
- *interaction.depth* - depth of a tree. I'll check 1, 2, or 4.
- *shrinkage* - learning rate parameter. Only 2 values of this parameter will be used: 0.01 and 0.1.
- *n.minobsinnode* - minimum number of observations in split. I'll try values between 10 and 25 (by 5).

```{r, train_gbm}
model_gbm <- caret::train(form = create_model_formula("quality", x_variables),
                          data = data_train, 
                          method = "gbm",
                          trControl = train_control_params, 
                          tuneGrid = expand.grid(n.trees = seq(100, 300, by = 50),
                                                 interaction.depth = c(1, 2, 4), 
                                                 shrinkage = c(0.01, 0.1), 
                                                 n.minobsinnode = seq(10, 25, 5)),
                          distribution = "gaussian",
                          verbose = FALSE)
```

### GBM tuning {.tabset}

#### Table

```{r, gbm_hyper}
table_with_highlight(model_gbm$results %>% top_n(n = 10, wt = -RMSE), "RMSE")
```

#### Plot

```{r, gbm_hyper_plot}
plot(model_gbm)
```

### {-}

As we can suspect the quality of the model gains as the number of trees increases (**ntree**). What is interesting that the behavior of **shrinkage** and **interaction.depth** parameters. In the end the combination of lower learning rate (**shrinkage** = 0.01) and higher depth of tree (**interaction.depth** = 4) turned out to be the best combination. Tuning of the last parameter (**n.minobsinnode**) resulted in value equal to 10.  

### Model results

What are the most important features?

```{r, var_imp_gbm}
var_imp_plot(data_ = summary.gbm(object = model_gbm$finalModel, plotit = FALSE), 
             var_name = "var", 
             var_imp_name = "rel.inf", plot_title = "Variable importance - GBM")
```

In GBM model the most important variables were **alcohol**, **feat07**, **volatile.acidity**, **feat04** and **sulphates**.

## eXtreme Gradient Boosting (XGBoost)

{caret} package enables to tune 7 parameters for XGBoost model. 4 of them

- *min_child_weight* - minimum number of observations in terminal node. Searching from 0.5% to 1% of number of observations (by 0.1%)
- *nrounds* - number of trees. Searching between 20 and 80 (by 10)
- *eta* - learning parameter. Searching between 0.01 to 0.15 (by 0.01)
- *colsample_bytree* - fraction of predictors to use. Searching between $\frac{\sqrt{21}}{21}$ ~ 21% to 61% (by 10%)

Other 3 will be set constant:

- *subsample* - size (in %) of subsample drawn from training dataset (set to 80)
- *gamma* - Minimum Loss Reduction (set to 1)
- *max_depth* - maximum depth of tree (set to 8)

```{r, traing_xgboost}
tune_params_xgboost <- list(colsample_bytree = seq(sqrt(length(x_variables)) / length(x_variables), 0.65, 0.1),
                            min_child_weight = nrow(data_train) * seq(.5, 1, .1) / 100,
                            nrounds = seq(20, 80, 10),
                            eta = seq(0.01, .15, by = .01))

static_params_xgboost <- list(subsample = 0.8, 
                              gamma = 1,
                              max_depth = 8)

file_to_look_for <- "model_xgboost_reg.rds"
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_xgboost <- caret::train(form = create_model_formula("quality", x_variables),
                                  data = data_train,
                                  method = "xgbTree",
                                  trControl = train_control_params, 
                                  tuneGrid = do.call("expand.grid", c(static_params_xgboost, tune_params_xgboost)),
                                  verbosity = 0)
    print(Sys.time() - start) # 3mins
    saveRDS(object = model_xgboost, file = file_to_look_for)
} else {
    model_xgboost <- readRDS(file = file_to_look_for)
}
```

### xGBoost tuning {.tabset}

#### Table

```{r, xgb_hyper}
table_with_highlight(model_xgboost$results %>% top_n(n = 10, wt = -RMSE), "RMSE")
```

#### Plot

```{r, xgb_hyper_plot}
plot(model_xgboost)
```

#### Non-standard plot

```{r, xgb_hyper_plot_other}
do.call(rbind, args = lapply(X = names(tune_params_xgboost), FUN = function(i) {
    model_xgboost$results %>% select(tune_val = i, meas_val = RMSE) %>% mutate(flag = i)
})) %>% 
    ggplot(mapping = aes(x = tune_val, y = meas_val)) + 
    geom_point() +
    facet_wrap(~flag, scales = "free_x") +
    geom_smooth(method = "loess", formula = "y ~ x") +
    labs(x = "Value of tuned parameter", y = "RMSE", title = "Values of tuned parameters vs RMSE") +
    theme_minimal()
```

### {-}

It seems that **colsample_bytree** and **min_child_weight** parameters didn't really matter. As suspected larger **eta** and **nrounds** lead to better results. But the best results were achieved with **eta** = 0.05, **nrounds** = 80, **colsample_bytree** = 0.41 and **min_child_weight** = 9.8. The best **eta** point could be observed as an elbow point in both standard (if displayed on large enough screen) and non-standard plots.

### Model results

What are the most important features?

```{r, var_imp_xgb}
var_imp_plot(data_ = varImp(model_xgboost)$importance %>% mutate(variable = row.names(.)) %>% filter(Overall > 0), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - xGBoost")
```

In xGBoost model the most important variables were **alcohol**, **feat07**, **volatile.acidity**, **feat04** and **sulphates**. It is the same set of features as in GBM model (slightly different order).

## Linear regression

As a benchmark model I'll used simple linear regression model.

```{r, train_lm}
model_linear_regr <- caret::train(form = create_model_formula("quality", x_variables), 
                                  data = data_train,
                                  method = "lm", 
                                  trControl = train_control_params)
```

# Ensembling and stacking

I'll use all the previous models for ensembling and stacking.

```{r, list_ensemble}
model_list <- caretEnsemble::caretList(create_model_formula("quality", x_variables),
                                       data = data_train,
                                       methodList = c("lm"),
                                       tuneList = list(
                                           rpart = caretModelSpec(method = "rpart", tuneGrid = model_dec_tree$bestTune), 
                                           rf = caretModelSpec(method = "rf", ntree = 200, tuneGrid = model_random_forest$bestTune),
                                           gbm = caretModelSpec(method = "gbm", distribution = "gaussian", verbose = FALSE, tuneGrid = model_gbm$bestTune),
                                           xgbTree = caretModelSpec(method = "xgbTree", verbosity = 0, tuneGrid = model_xgboost$bestTune)
                                       ),
                                       trControl = train_control_params)

```

What is the correlations between models?

## Correlation {.tabset}

### Plot

```{r, correlation_between_models_plot}
corrplot::corrplot(modelCor(resamples(model_list)), method = "number")
```

### Table

```{r, correlation_between_models_table}
modelCor(resamples(model_list)) %>% 
    as.data.frame() %>% 
    mutate(id = row.names(.)) %>% 
    pivot_longer(cols = setdiff(names(.), "id")) %>% 
    mutate(p1 = pmin(id, name), p2 = pmax(id, name)) %>% 
    select(p1, p2, value) %>% 
    unique() %>% 
    filter(p1 != p2) %>% 
    top_n(n = 4, wt = abs(value)) %>% 
    arrange(-value) %>% 
    kable(col.names = c("Model 1", "Model 2", "Correlation coeff.")) %>% 
    kable_styling()
```

## {-}

The highest correlation (in absolute terms) are between:

- xGBoost and GBM: 0.9905
- xGBoost and Random Forest: 0.9500
- GBM and Linear Regression: 0.9469
- Linear Regression and xGBoost: 0.9291

Those findings are consistent with what was seen when analyzing each model separately.

## Ensembling

```{r, train_ensemble}
model_ensembled <- caretEnsemble::caretEnsemble(model_list)
summary(model_ensembled)
```

By default {caret} packages uses linear regression to determine the weights for each model. Lets look at variable importance:

```{r, var_imp_ensemble}
res_var_imp_ens <- varImp(model_ensembled) %>% 
    mutate(variable = rownames(.)) %>% 
    gather("model", "importance", -variable)

imp_lvls <- res_var_imp_ens %>% 
    filter(model == "overall")  %>% top_n(n = 10, wt = importance) %>% arrange(importance) %>% .$variable

res_var_imp_ens %>% 
    filter(variable %in% imp_lvls) %>% 
    mutate(variable2 = factor(x = variable, levels = imp_lvls)) %>%
    ggplot(mapping = aes(x = variable2, y = importance, color = model)) +
    geom_point(mapping = aes(shape = model == "overall", 
                             size = model == "overall")) +
    coord_flip() +
    theme_minimal() +
    scale_size_manual(guide = "none", values = c("TRUE" = 4, "FALSE" = 3)) +
    scale_color_viridis_d() +
    scale_shape_manual(values = c("TRUE" = 5, "FALSE" = 16)) +
    labs(x = "variables", color = "Model", shape = "Is it overall\nmodel?",
         title = "Variable importance from ensemble model", y = "Variable importance") +
    theme(legend.position = "bottom")
```

As we clearly see the Decision Tree model picked substantially different variables as the most important.

## Stacking 

As stack model I'll use XGBoost and Lasso Regression as top layer models for comparison.

```{r, train_model_stacked_xgb}
stacked_tr <- trainControl(method = "boot", 
                           number = 10,
                           savePredictions = "final",
)
model_stacked_xgb <- caretEnsemble::caretStack(model_list,
                                               method = "xgbTree",
                                               trControl = stacked_tr)
```

```{r, train_model_stacked_lm}
model_stacked_lasso <- caretEnsemble::caretStack(model_list,
                                                 method = "lasso",
                                                 trControl = stacked_tr)
```

# Model assessment 

As an assessment methods I'll use RMSE. First, define model list:

```{r, define_models_for_comparison}
model_list_specs <- list("Decision tree" = list(model_n = "model_dec_tree"),
                         "Random forest" = list(model_n = "model_random_forest"),
                         "GBM"           = list(model_n = "model_gbm"),
                         "XGBoost"       = list(model_n = "model_xgboost"),
                         "Linear"        = list(model_n = "model_linear_regr"),
                         "Ensembled"     = list(model_n = "model_ensembled"),
                         "Stacked (XGB)" = list(model_n = "model_stacked_xgb"),
                         "Stacked (Lasso)"  = list(model_n = "model_stacked_lasso"))
```

## Comparison {.tabset}

### RMSE on training data

```{r, compare_train}
compare_models(model_list_specs, data_train, "quality")
```

### RMSE on testing data

```{r, compare_test}
compare_models(model_list_specs, data_test, "quality")
```

### Table

```{r, comparison_table}
summ_up_table <- do.call(what = "rbind", 
                         args = lapply(X = names(model_list_specs), 
                                       FUN = function(i){
                                           data.frame(name = i, 
                                                      rmse_train = rmse_model(model = get(model_list_specs[[i]]$model_n), data_ = data_train, ys = data_train[[y_variable]]),
                                                      rmse_test = rmse_model(model = get(model_list_specs[[i]]$model_n), data_ = data_test, ys = data_test[[y_variable]]))
                                       })) %>% 
    arrange(., abs(rmse_train - rmse_test)) 

saveRDS(object = summ_up_table, file = "regr_summ_up_table.rds")

summ_up_table %>% 
    mutate(rank_train = rank(rmse_train), rank_test = rank(rmse_test)) %>% 
    kable(booktabs = T, col.names = c("Model", "RMSE (Train)", "RMSE (Test)", "Rank (Train)", "Rank (Test)")) %>%
    kable_styling() %>%
    row_spec(which(summ_up_table$rmse_train == min(summ_up_table$rmse_train)), bold = T, color = "white", background = "green") %>% 
    row_spec(which(summ_up_table$rmse_test == min(summ_up_table$rmse_test)), underline = T)
```

## Results

Smallest RMSE on train data was recorded in Random Forest (0.43), largest in Decision Tree (1.1). On testing data smallest RMSE characterized Ensembled model (1.13), largest: Decision Tree (1.24).
Smallest overfit was found in Linear Regressien (as measure by the change between RMSE in training vs testing dataset). 

# Summary and conclusions

Ensemble model was the best model - it had 3rd lowest RMSE on training data and 1st lowest on testing. 
- Stacking with linear regression yields same (ish) results as ensembling
- alcohol as most dominant feature
