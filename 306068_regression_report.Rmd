---
title: "Red Wine Quality"
subtitle: "Predicting using regression methods"
output: 
    html_document:
        toc: true
        toc_float: true
        smooth_scroll: true
---

```{r setup, include=FALSE}
# libraries
library(tidyverse) 
library(corrplot) 
library(kableExtra) 
library(caret) 
library(caretEnsemble) 
library(rpart)
library(randomForest)
library(gbm)
library(xgboost)

# settings
knitr::opts_chunk$set(cache = FALSE)
options(scipen = 999)
store_results_in <- "regr_results/"
dir.create(store_results_in, showWarnings = FALSE, recursive = TRUE)

# functions
source("__functions.R")
```

# Description of data and problem
## Data
Data regards Vinho Verde. Load the dataset and take a quick look at the dataset structure.

```{r, glimpse_at_data}
data_raw <- read_csv(file = "r2.csv", show_col_types = FALSE)
glimpse(data_raw)
```

1,400 observations, 23 variables.

## Problem

Predict wine quality based on its features.

# Descriptive analyses

Start with establishing dependent variable (**y_variable**) and predictors (**x_variables**).

```{r, data_cleaning}
# Change the column names to avoid possible problems in the future
names(data_raw) <- make.names(names(data_raw))
# 'quality' as dependent variable
y_variable <- "quality"
# rest (excluding 'id') are predictors
x_variables <- setdiff(names(data_raw), c("id", y_variable))
```

Quick look at the dependent variable:

```{r, y_distribution}
ggplot(data = data_raw, mapping = aes(x = .data[[y_variable]])) + 
    geom_histogram(mapping = aes(y = after_stat(density)), binwidth = .5) +
    geom_density(alpha = .4, fill = "darkblue", bw = 2.5) +
    theme_minimal() +
    scale_x_continuous(breaks = floor(min(data_raw$quality)):ceiling(max(data_raw$quality))) +
    labs(title = "Distribution of wine quality", x = "Quality", y = "Density", subtitle = "bins: observed, line: estimated")
```

It takes values from 1.8 up to 10.6 (mean = 5.7). Take a look at continuous variables:

```{r, descr_cont}
table_descr_cont <- do.call(what = rbind, args = lapply(X = x_variables, FUN = function(i) describe_cont_var(data_raw, i)))

saveRDS(object = table_descr_cont, file = paste0(store_results_in, "regr_table_descr_cont.rds"))

table_descr_cont %>% 
    mutate(min = round(min, 2), mean = round(mean, 2), sd = round(sd, 2), max = round(max, 2)) %>% 
    kable(booktabs = T, col.names = c("Variable", "Min.", "Mean", "SD", "Max.", "# missing", "# unique")) %>%
    kable_styling()
```

- no need for imputation (no missing records)
- minimum number of distinct values is 56 - no clear indication of turning variable into discret one.

Is there a correlation between variables?

```{r, corrplot}
corrplot::corrplot(corr = cor(data_raw[, c(x_variables, y_variable)]), method = "color")
```

- high correlation between *pH* and *fixed.acidicity*
- weak correlation between *citric.acid* and *violatile.acidicity*, *pH*
- no clear correlation between outcome variable and predictors except for *feat04* and *feat06*

## EDA {.tabset}

### alcohol

```{r, alcohol_plot}
create_mean_plot(data_ = data_raw, col_name_ = "alcohol", q_breaks = c(-Inf, 10, 11, 12, Inf))
```

There is a weak positive correlation between *alcohol* and *quality*. I've decided to split this variable into 4 bins.

```{r, alcohol_transform}
data_raw <- data_raw %>% 
    mutate(trans.alcohol = as.character(as.numeric(cut(x = alcohol, c(-Inf, 10, 11, 12, Inf)), include.lowest = T)))
```

### chlorides

```{r, chlorides_plot}
create_mean_plot(data_ = data_raw, col_name_ = "chlorides", q_breaks = c(-Inf, 0.075, 0.1, 0.15, Inf)) + 
    scale_x_log10() + labs(x = "chlorides (log)")
```

There is a weak negative correlation between *chlorides* and *quality*. I've decided to split this variable into 4 bins.

```{r, chlorides_transform}
data_raw <- data_raw %>% 
    mutate(trans.chlorides = as.character(as.numeric(cut(x = chlorides, c(-Inf, 0.075, 0.1, 0.15, Inf)), include.lowest = T)))
```

### citric.acid

```{r, citric.acid_plot}
create_mean_plot(data_ = data_raw, col_name_ = "citric.acid", q_breaks = c(-Inf, .25, .5, Inf))
```

There is a weak positive correlation between *citric.acid* and *quality*. I've decided to split this variable into 3 bins.

```{r, citric.acid_transform}
data_raw <- data_raw %>% 
    mutate(trans.citric.acid = as.character(as.numeric(cut(x = citric.acid, c(-Inf, .25, .5, Inf)), include.lowest = T)))
```

### density

```{r, density_plot}
create_mean_plot(data_ = data_raw, col_name_ = "density", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *density* and *quality*. I've decided to remove that variable.

```{r, density_transform}
x_variables <- setdiff(x_variables, "density")
```

### feat01

```{r, feat01_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat01", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *feat01* and *quality*. I've decided to remove that variable.

```{r, feat01_transform}
x_variables <- setdiff(x_variables, "feat01")
```

### feat02

```{r, feat02_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat02", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *feat02* and *quality*. I've decided to remove that variable.

```{r, feat02_transform}
x_variables <- setdiff(x_variables, "feat02")
```

### feat03

```{r, feat03_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat03", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *feat03* and *quality*. I've decided to remove that variable.

```{r, feat03_transform}
x_variables <- setdiff(x_variables, "feat03")
```

### feat04

```{r, feat04_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat04", q_breaks = c(-Inf, .4, .65, .7, Inf))
```

There is weak positive correlation between *feat04* and *quality*. I've decided to split this variable into 4 bins.

```{r, feat04_transform}
data_raw <- data_raw %>% 
    mutate(trans.feat04 = as.character(as.numeric(cut(x = feat04, c(-Inf, .4, .65, .7, Inf)), include.lowest = T)))
```

### feat05

```{r, feat05_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat05", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *feat05* and *quality*. I've decided to remove that variable.

```{r, feat05_transform}
x_variables <- setdiff(x_variables, "feat05")
```

### feat06

```{r, feat06_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat06", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *feat06* and *quality*. I've decided to remove that variable.

```{r, feat06_transform}
x_variables <- setdiff(x_variables, "feat06")
```

### feat07

```{r, feat07}
create_mean_plot(data_ = data_raw, col_name_ = "feat07", q_breaks = c(-Inf, .5, .7, Inf))
```

There is a weak positive correlation between *feat07* and *quality*. I've decided to split this variable into 3 bins.

```{r, feat07_transform}
data_raw <- data_raw %>% 
    mutate(trans.feat07 = as.character(as.numeric(cut(x = feat07, c(-Inf, .5, .7, Inf)), include.lowest = T)))
```

### feat08

```{r, feat08_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat08", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *feat08* and *quality*. I've decided to remove that variable.

```{r, feat08_transform}
x_variables <- setdiff(x_variables, "feat08")
```

### feat09

```{r, feat09_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat09", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *feat09* and *quality*. I've decided to remove that variable.

```{r, feat09_transform}
x_variables <- setdiff(x_variables, "feat09")
```

### feat10

```{r, feat10_plot}
create_mean_plot(data_ = data_raw, col_name_ = "feat10", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *feat10* and *quality*. I've decided to remove that variable.

```{r, feat10_transform}
x_variables <- setdiff(x_variables, "feat10")
```


### fixed.acidity

```{r, fixed.acidity_plot}
create_mean_plot(data_ = data_raw, col_name_ = "fixed.acidity", q_breaks = c(-Inf, 7, 8, 9, 10, 11, Inf))
```

There is a weak quadratic relation between *fixed.acidity* and *quality*. I've decided to split this variable into 6 bins.

```{r, fixed.acidity_transform}
data_raw <- data_raw %>% 
    mutate(trans.fixed.acidity = as.character(as.numeric(cut(x = fixed.acidity, c(-Inf, 7, 8, 9, 10, 11, Inf)), include.lowest = T)))
```

### free.sulfur.dioxide

```{r, free.sulfur.dioxide_plot}
create_mean_plot(data_ = data_raw, col_name_ = "free.sulfur.dioxide", q_breaks = c(-Inf, 20, Inf))
```

There is a weak negative relation between *free.sulfur.dioxide* and *quality*. I've decided to split this variable into 2 bins.

```{r, free.sulfur.dioxide_transform}
data_raw <- data_raw %>% 
    mutate(trans.free.sulfur.dioxide = as.character(as.numeric(cut(x = free.sulfur.dioxide, c(-Inf, 20, Inf)), include.lowest = T)))
```

### pH

```{r, pHplot}
create_mean_plot(data_ = data_raw, col_name_ = "pH", q_breaks = c(-Inf, 3.2, 3.4, Inf))
```

There is a weak quadratic relation between *pH* and *quality*. I've decided to split this variable into 3 bins.

```{r, pH_transform}
data_raw <- data_raw %>% 
    mutate(trans.pH = as.character(as.numeric(cut(x = pH, c(-Inf, 3.2, 3.4, Inf)), include.lowest = T)))
```

### residual.sugar

```{r, residual.sugar_plot}
create_mean_plot(data_ = data_raw, col_name_ = "residual.sugar", q_breaks = c(-Inf, Inf))
```

There is no clear correlation between *residual.sugar* and *quality*. I've decided to remove that variable.

```{r, residual.sugar_transform}
x_variables <- setdiff(x_variables, "residual.sugar")
```

### sulphates

```{r, sulphates_plot}
create_mean_plot(data_ = data_raw, col_name_ = "sulphates", q_breaks = c(-Inf, 4.5, 4.6, 4.7, 5, Inf))
```

There is a weak quadratic relation between *sulphates* and *quality*. I've decided to split this variable into 5 bins.

```{r, sulphates_transform}
data_raw <- data_raw %>% 
    mutate(trans.sulphates = as.character(as.numeric(cut(x = sulphates, c(-Inf, 4.5, 4.6, 4.7, 5, Inf)), include.lowest = T)))
```

### total.sulfur.dioxide

```{r, total.sulfur.dioxide_plot}
create_mean_plot(data_ = data_raw, col_name_ = "total.sulfur.dioxide", q_breaks = c(-Inf, Inf))
```

There is a weak negative relation between *sulphates* and *quality*. I've decided to leave that variable as is.

### volatile.acidity

```{r, volatile.acidity_plot}
create_mean_plot(data_ = data_raw, col_name_ = "volatile.acidity", q_breaks = c(-Inf, Inf))
```

There is a weak negative relation between *volatile.acidity* and *quality*. I've decided to leave that variable as is.

# Data preparation 

After initial EDA I've decided to:

- bin several variables into discrete
- remove variable with no correlation to *quality*
- leave some variables as they were (for project sake)

Discrete variables will be decoded as binary:

```{r, one_hot}
x_variables_dsc <- names(data_raw)[which(regexpr(pattern = "^trans", text = names(data_raw)) > 0)]
# fullRank = TRUE takes care of linear dependency of variables
one_hot_data <- data.frame(predict(object = dummyVars(formula = " ~ .", data = data_raw[x_variables_dsc], fullRank = TRUE), 
                                   newdata = data_raw[x_variables_dsc]))
```

Bind all in one object:

```{r, create_new_data}
x_variables_cont <- setdiff(x_variables, gsub(pattern = "^trans\\.", replacement = "", x = x_variables_dsc))
new_data_raw <- cbind(data_raw[c(y_variable, x_variables_cont)], one_hot_data)
new_x_variables <- setdiff(names(new_data_raw), y_variable)
```

Final dataset consists 27 variables.

## Creating training and testing dataset

70/30 partition into training/testing datasets will be used.

```{r, data_partitioning}
set.seed(306068)
training_obs <- caret::createDataPartition(new_data_raw$quality, 
                                           p = 0.7, 
                                           list = FALSE)
data_train <- new_data_raw[training_obs,]
data_test <- new_data_raw[-training_obs,]
```

Additionally 5-time Cross validation will be used.

```{r, cv_params}
train_control_params <- trainControl(method = "cv", 
                                     number = 5,
                                     savePredictions = "all")
```

## Variable transformations

Normalize continuous variables (based on parameters form training data)

```{r, data_normalization}
means <- apply(X = data_train[, x_variables_cont], MARGIN = 2, FUN = mean)
sds <- apply(X = data_train[, x_variables_cont], MARGIN = 2, FUN = sd)
data_train[, x_variables_cont] <- lapply(X = x_variables_cont, FUN = function(i) (data_train[[i]] - means[i]) / sds[i])
data_test[, x_variables_cont] <- lapply(X = x_variables_cont, FUN = function(i) (data_test[[i]] - means[i]) / sds[i])
```

# Model training

## Decision tree

First model to train will be a **decision tree**. Apart from using default {caret} parameters I'll add hyperparametr: **cp**. It determines the complexity of a tree it impose restrictions on tree growth. 

```{r, train_dec_tree}
model_dec_tree <- caret::train(form = create_model_formula("quality", new_x_variables), 
                               data = data_train, 
                               method = "rpart",
                               trControl = train_control_params, 
                               tuneGrid = data.frame(cp = seq(0, .05, .005))
)
```

What is the best value of **cp** parameter?

### cp tuning {.tabset}

#### Table

```{r, dec_tree_hyper}
table_with_highlight(data_ = model_dec_tree$results, col_name = "RMSE")
```

#### Plot

```{r, dec_tree_hyper_plot}
plot(model_dec_tree)
```

### {-}

Best value for **cp** is **0.015** which means that any split that does not decrease overall lack of fit by a factor of 0.015 is not attempted.

### Model results

Take a look at the splits of the tree.

```{r, dec_tree_splits}
rpart.plot::rpart.plot(model_dec_tree$finalModel)
```

It is a simple tree: first split is done according to the **volatile.acidity** value, in next either **feat04** or **alcohol** is used. Beside the split we should look into variable importance:

```{r, var_imp_dec_tree}
var_imp_plot(data_ = varImp(model_dec_tree$finalModel) %>% mutate(variable = row.names(.)) %>% filter(Overall > 0), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - Decision Tree")
```

**total.sulfur.dioxide**, **alcohol** and **sulphates** seems to be the most important variables.

## Random forest

Next model to use is random forest. I'll change number of trees (**ntree**) from 500 to 200. Additionally I'll try to find the optimal value of **mtry** parameter. In order to do so I'll search the values between 4 and 10 (the default value is square root from the number of predictors - in ths case $\sqrt{27} = 5.2$)

```{r, train_random_forest}
model_random_forest <- caret::train(form = create_model_formula("quality", new_x_variables),
                                    data = data_train, 
                                    method = "rf", 
                                    ntre = 200,
                                    trControl = train_control_params, 
                                    tuneGrid = data.frame(mtry = 5:10))
```

### mtry tuning {.tabset}

#### Table

```{r, rf_hyper}
table_with_highlight(model_random_forest$results, "RMSE")
```

#### Plot

```{r, rf_hyper_plot}
plot(model_random_forest)
```

### {-}

Best value for **ntry** is **5** which means that for each of the trees only 5 randomly selected predictors will be used.

### Model results

Which features are the most important?

```{r, var_imp_rf}
var_imp_plot(data_ = varImp(model_random_forest)$importance %>% mutate(variable = row.names(.)) %>% filter(Overall > 0), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - Random Forest")
```

**volatile.acidity**, **total.sulfur.dioxide** and **alcohol** and  are the most important variables.

## Generalized Boosted Regression Modeling (GBM)

Next model is GBM. In this case 4 hyperparameters will be optimized:

- *ntree* - number of trees. Values from 100 to 300 (by 50) will be checked.
- *interaction.depth* - depth of a tree. I'll check 1, 2, or 4.
- *shrinkage* - learning rate parameter. Only 2 values of this parameter will be used: 0.01 and 0.1.
- *n.minobsinnode* - minimum number of observations in split. I'll try values between 10 and 25 (by 5).

```{r, train_gbm}
model_gbm <- caret::train(form = create_model_formula("quality", new_x_variables),
                          data = data_train, 
                          method = "gbm",
                          trControl = train_control_params, 
                          tuneGrid = expand.grid(n.trees = seq(100, 300, by = 50),
                                                 interaction.depth = c(1, 2, 4), 
                                                 shrinkage = c(0.01, 0.1), 
                                                 n.minobsinnode = seq(10, 25, 5)),
                          distribution = "gaussian",
                          verbose = FALSE)
```

### GBM tuning {.tabset}

#### Table

```{r, gbm_hyper}
table_with_highlight(model_gbm$results %>% top_n(n = 10, wt = -RMSE), "RMSE")
```

#### Plot

```{r, gbm_hyper_plot}
plot(model_gbm)
```

### {-}

As we can suspect the quality of the model gains as the number of trees increases (**ntree**). What is interesting that the behavior of **shrinkage** and **interaction.depth** parameters. In the end the combination of higher learning rate (**shrinkage** = 0.1) and lower depth of tree (**interaction.depth** = 1) turned out to be the best combination. Tuning of the last parameter (**n.minobsinnode**) resulted in value equal to 25.  

### Model results

What are the most important features?

```{r, var_imp_gbm}
var_imp_plot(data_ = summary.gbm(object = model_gbm$finalModel, plotit = FALSE), 
             var_name = "var", 
             var_imp_name = "rel.inf", plot_title = "Variable importance - GBM")
```

In GBM model the most important variables were **volatile.acidity**, **total.sulfur.dioxide** and **alcohol**.

## eXtreme Gradient Boosting (XGBoost)

{caret} package enables to tune 7 parameters for XGBoost model. 4 of them

- *min_child_weight* - minimum number of observations in terminal node. Searching from 0.5% to 1% of number of observations (by 0.1%)
- *nrounds* - number of trees. Searching between 20 and 80 (by 10)
- *eta* - learning parameter. Searching between 0.01 to 0.15 (by 0.01)
- *colsample_bytree* - fraction of predictors to use. Searching between $\frac{\sqrt{27}}{27}$ ~ 19% to 59% (by 10%)

Other 3 will be set constant:

- *subsample* - size (in %) of subsample drawn from training dataset (set to 80)
- *gamma* - Minimum Loss Reduction (set to 1)
- *max_depth* - maximum depth of tree (set to 8)

```{r, traing_xgboost}
tune_params_xgboost <- list(min_child_weight = nrow(data_train) * seq(.5, 1, .1) / 100,
                            nrounds = seq(20, 80, 10),
                            eta = seq(0.01, .15, by = .01),
                            colsample_bytree = seq(sqrt(length(new_x_variables)) / length(new_x_variables), 0.60, 0.1))

static_params_xgboost <- list(subsample = 0.8, 
                              gamma = 1,
                              max_depth = 8)

file_to_look_for <- paste0(store_results_in, "model_xgboost_reg.rds")
if (!file.exists(file_to_look_for)) {
    start <- Sys.time()
    model_xgboost <- caret::train(form = create_model_formula("quality", new_x_variables),
                                  data = data_train,
                                  method = "xgbTree",
                                  trControl = train_control_params, 
                                  tuneGrid = do.call("expand.grid", c(static_params_xgboost, tune_params_xgboost)),
                                  verbosity = 0)
    print(Sys.time() - start) # 2mins
    saveRDS(object = model_xgboost, file = file_to_look_for)
} else {
    model_xgboost <- readRDS(file = file_to_look_for)
}
```

### xGBoost tuning {.tabset}

#### Table

```{r, xgb_hyper}
table_with_highlight(model_xgboost$results %>% top_n(n = 10, wt = -RMSE), "RMSE")
```

#### Plot

```{r, xgb_hyper_plot}
plot(model_xgboost)
```

#### Non-standard plot

```{r, xgb_hyper_plot_other}
do.call(rbind, args = lapply(X = names(tune_params_xgboost), FUN = function(i) {
    model_xgboost$results %>% select(tune_val = i, meas_val = RMSE) %>% mutate(flag = i)
})) %>% 
    ggplot(mapping = aes(x = tune_val, y = meas_val)) + 
    geom_point() +
    facet_wrap(~flag, scales = "free_x") +
    geom_smooth(method = "loess", formula = "y ~ x") +
    labs(x = "Value of tuned parameter", y = "RMSE", title = "Values of tuned parameters vs RMSE") +
    theme_minimal()
```

### {-}

It seems that **colsample_bytree** and **min_child_weight** parameters didn't really matter. As suspected larger **eta** and **nrounds** lead to better results. But the best results were achieved with **eta** = 0.08, **nrounds** = 60, **colsample_bytree** = 0.29 and **min_child_weight** = 5.9. 

### Model results

What are the most important features?

```{r, var_imp_xgb}
var_imp_plot(data_ = varImp(model_xgboost)$importance %>% mutate(variable = row.names(.)) %>% filter(Overall > 0), 
             var_name = "variable", 
             var_imp_name = "Overall", plot_title = "Variable importance - xGBoost")
```

In xGBoost model the most important variables were **total.sulfur.dioxide**, **volatile.acidity** and **sulphates**.

## Linear regression

As a benchmark model I'll used simple linear regression model.

```{r, train_lm}
model_linear_regr <- caret::train(form = create_model_formula("quality", new_x_variables), 
                                  data = data_train,
                                  method = "lm", 
                                  trControl = train_control_params)
```

# Ensembling and stacking

I'll use all the previous models for ensembling and stacking.

```{r, list_ensemble}
model_list <- caretEnsemble::caretList(create_model_formula("quality", new_x_variables),
                                       data = data_train,
                                       methodList = c("lm"),
                                       tuneList = list(
                                           rpart = caretModelSpec(method = "rpart", tuneGrid = model_dec_tree$bestTune), 
                                           rf = caretModelSpec(method = "rf", ntree = 200, tuneGrid = model_random_forest$bestTune),
                                           gbm = caretModelSpec(method = "gbm", distribution = "gaussian", verbose = FALSE, tuneGrid = model_gbm$bestTune),
                                           xgbTree = caretModelSpec(method = "xgbTree", verbosity = 0, tuneGrid = model_xgboost$bestTune)
                                       ),
                                       trControl = train_control_params)

```

What is the correlations between models?

## Correlation {.tabset}

### Plot

```{r, correlation_between_models_plot}
corrplot::corrplot(modelCor(resamples(model_list)), method = "number")
```

### Table

```{r, correlation_between_models_table}
modelCor(resamples(model_list)) %>% 
    as.data.frame() %>% 
    mutate(id = row.names(.)) %>% 
    pivot_longer(cols = setdiff(names(.), "id")) %>% 
    mutate(p1 = pmin(id, name), p2 = pmax(id, name)) %>% 
    select(p1, p2, value) %>% 
    unique() %>% 
    filter(p1 != p2) %>% 
    top_n(n = 4, wt = abs(value)) %>% 
    arrange(-value) %>% 
    kable(col.names = c("Model 1", "Model 2", "Correlation coeff.")) %>% 
    kable_styling()
```

## {-}

The highest correlation (in absolute terms) are between Linear Regression-Random Forest and GBM-XGBoost. Those findings are consistent with what was seen when analyzing each model separately.

## Ensembling

```{r, train_ensemble}
model_ensembled <- caretEnsemble::caretEnsemble(model_list)
summary(model_ensembled)
```

By default {caret} packages uses linear regression to determine the weights for each model. Lets look at variable importance:

```{r, var_imp_ensemble}
res_var_imp_ens <- varImp(model_ensembled) %>% 
    mutate(variable = rownames(.)) %>% 
    gather("model", "importance", -variable)

imp_lvls <- res_var_imp_ens %>% 
    filter(model == "overall") %>% top_n(n = 10, wt = importance) %>% arrange(importance) %>% .$variable

res_var_imp_ens %>% 
    filter(variable %in% imp_lvls) %>% 
    mutate(variable2 = factor(x = variable, levels = imp_lvls)) %>%
    ggplot(mapping = aes(x = variable2, y = importance, color = model)) +
    geom_point(mapping = aes(shape = model == "overall", 
                             size = model == "overall")) +
    coord_flip() +
    theme_minimal() +
    scale_size_manual(guide = "none", values = c("TRUE" = 4, "FALSE" = 3)) +
    scale_color_viridis_d() +
    scale_shape_manual(values = c("TRUE" = 5, "FALSE" = 16)) +
    labs(x = "variables", color = "Model", shape = "Is it overall\nmodel?",
         title = "Variable importance from ensemble model", y = "Variable importance") +
    theme(legend.position = "bottom")
```

In the ensembled model most important variables were **volatile.acidity**, **totat.sulfur.dioxide** and level 4 of **alcohol**.

## Stacking 

As stack model I'll use XGBoost and Lasso Regression as top layer models for comparison.

```{r, train_model_stacked_xgb}
stacked_tr <- trainControl(method = "boot", 
                           number = 10,
                           savePredictions = "final",
)
model_stacked_xgb <- caretEnsemble::caretStack(model_list,
                                               method = "xgbTree",
                                               trControl = stacked_tr)
```

```{r, train_model_stacked_lm}
model_stacked_lasso <- caretEnsemble::caretStack(model_list,
                                                 method = "lasso",
                                                 trControl = stacked_tr)
```

# Model assessment 

As an assessment methods I'll use RMSE. First, define model list:

```{r, define_models_for_comparison}
model_list_specs <- list("Decision tree"   = list(model_n = "model_dec_tree"),
                         "Random forest"   = list(model_n = "model_random_forest"),
                         "GBM"             = list(model_n = "model_gbm"),
                         "XGBoost"         = list(model_n = "model_xgboost"),
                         "Linear"          = list(model_n = "model_linear_regr"),
                         "Ensembled"       = list(model_n = "model_ensembled"),
                         "Stacked (XGB)"   = list(model_n = "model_stacked_xgb"),
                         "Stacked (Lasso)" = list(model_n = "model_stacked_lasso"))
```

Then plot those models against **quality** variable to additionally see how the models performed.

## Comparison {.tabset}

### RMSE on training data

```{r, compare_train}
compare_models(model_list_specs, data_train, "quality")
```

### RMSE on testing data

```{r, compare_test}
compare_models(model_list_specs, data_test, "quality")
```

### Table

```{r, comparison_table}
summ_up_table <- do.call(what = "rbind", 
                         args = lapply(X = names(model_list_specs), 
                                       FUN = function(i){
                                           data.frame(name = i, 
                                                      rmse_train = rmse_model(model = get(model_list_specs[[i]]$model_n), data_ = data_train, ys = data_train[[y_variable]]),
                                                      rmse_test = rmse_model(model = get(model_list_specs[[i]]$model_n), data_ = data_test, ys = data_test[[y_variable]]))
                                       })) %>% 
    arrange(., abs(rmse_train - rmse_test)) 

saveRDS(object = summ_up_table, file = paste0(store_results_in, "regr_summ_up_table.rds"))

summ_up_table %>% 
    mutate(rank_train = rank(rmse_train), rank_test = rank(rmse_test)) %>% 
    kable(booktabs = T, col.names = c("Model", "RMSE (Train)", "RMSE (Test)", "Rank (Train)", "Rank (Test)")) %>%
    kable_styling() %>%
    row_spec(which(summ_up_table$rmse_train == min(summ_up_table$rmse_train)), bold = T, color = "white", background = "green") %>% 
    row_spec(which(summ_up_table$rmse_test == min(summ_up_table$rmse_test)), underline = T)
```

## Results

Smallest RMSE on train data was recorded in Random Forest (0.63), largest in Decision Tree (1.1). On testing data smallest RMSE characterized Stacked model using Lasso Regression (1.13), largest: Decision Tree (1.23).
Smallest overfit was found in Linear Regressien (as measure by the change between RMSE in training vs testing dataset). 

# Summary and conclusions

- Stacked model was the best model - it had 3rd lowest RMSE on training data and 1st lowest on testing
- Random Forest was quite overfit, despite the fact that several multiple continuous variables were transformed into discrete
- from the artificial variables only **feat04** and **feat07** were used
- most important features overall were
    - **volatile.acidity**
    - **total.sulfur.dioxide**
    - **trans.alcohol4** which translates to value of **alcohol** between 12.1 and 15
    - **trans.feat072** which translates to values of **feat07** between 0.5 and 0.7
